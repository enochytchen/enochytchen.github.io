[{"authors":["admin"],"categories":null,"content":"I am a Research Assistant at the Biostatistics Group at the Department of Medical Epidemiology and Biostatistics, Karolinska Institutet. My main research interests are developing and applying statistical approaches for register-based epidemiological studies, particularly in survival analysis and cost-effectiveness studies in health technology assessment. I enjoy teaching statistics and telling stories as well. I truly hope you find my personal website useful to you.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://enochytchen.com/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a Research Assistant at the Biostatistics Group at the Department of Medical Epidemiology and Biostatistics, Karolinska Institutet. My main research interests are developing and applying statistical approaches for register-based epidemiological studies, particularly in survival analysis and cost-effectiveness studies in health technology assessment.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"This is a testing page for using Stata Markdown.  I followed GR\u0026rsquo;s Website using markstat to regenerate the Italy course materials. The logic of file organisation is similar to Biostat III.\n","date":1603670400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1603670400,"objectID":"0ea7fadc90f37875a3b7e2f3db5263f7","permalink":"https://enochytchen.com/cansurv/","publishdate":"2020-10-26T00:00:00Z","relpermalink":"/cansurv/","section":"cansurv","summary":"This is just a testing page for using Stata Markdown.","tags":null,"title":"Statistical methods for population-based cancer survival analysis","type":"book"},{"authors":null,"categories":null,"content":"","date":1601424000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1601424000,"objectID":"24f14dc57e687cfc7e09f040377871d3","permalink":"https://enochytchen.com/courses/biostatbasics/","publishdate":"2020-09-30T00:00:00Z","relpermalink":"/courses/biostatbasics/","section":"courses","summary":"(4FH089) For MSc students in Public Health Sciences at Karolinska Institutet, Spring 2021.","tags":null,"title":"Basic Biostatistics","type":"book"},{"authors":null,"categories":null,"content":"Stories behind this tutorial I wrote these tutorial while conducting my MSc thesis project in validating different survival extrapolation methods. During my work as a master student at Karolinska Institutet, I especially benefitted from Paul Dickman\u0026rsquo;s Stata teaching tutorials, Therese Andersson\u0026rsquo;s article on Statistics in Medicine (2013) and her PhD thesis, Paul Lambert\u0026rsquo;s Interactive graphs of explaining spline functions , Sandra Eloranta and Hannah Bower\u0026rsquo;s previous analysis files. It took me time and effort to find the right way to organize and generate the information needed for extrapolation. Thus, I would like to share with people who are eager to understand how to extrapolate survival and estimate life expectancy by using flexible parametric models (stpm2 package in Stata).\n","date":1588118400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1588118400,"objectID":"d73c108959a4b69e927b2353bbeddaab","permalink":"https://enochytchen.com/tutorials/extrapolation/","publishdate":"2020-04-29T00:00:00Z","relpermalink":"/tutorials/extrapolation/","section":"tutorials","summary":"Learn how to extrapolate survival data in Stata","tags":null,"title":"Survival extrapolation using Stata","type":"docs"},{"authors":null,"categories":null,"content":"Introduction ","date":1601769600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1601769600,"objectID":"dc39377c4f3d655e037f2fd71367eb59","permalink":"https://enochytchen.com/tutorials/learningrs/","publishdate":"2020-10-04T00:00:00Z","relpermalink":"/tutorials/learningrs/","section":"tutorials","summary":"Relative survival is such a magic to understand survival, particularly for cancer epidemiological research. Do you want to be a wizard together?","tags":null,"title":"Learning relative survival with me","type":"docs"},{"authors":null,"categories":null,"content":"Introduction Tied event times in survival mean that some individuals have the same survival times (1), though in reality lifetime is a continuous variable; that is, no individual has exactly the same survival times. However, in practice, cancer registers often only provide data on discrete survival times (for example, in months or in years) (2), which often results in some individuals having identical (tied) survival time.\nPohar-Perme estimator is a non-parametric estimate of relative survival. It is distinct from other approaches (Ederer I, Ederer II, and Hakulinen), by weighting by the inverse of the individual\u0026rsquo;s expected survival (3), which makes it an unbiased estimator. For estimating relative survival using Pohar-Perme estimator, there are already multiple packages developed in R and Stata . In rs.surv, stns, stpp, survival time is treated continuosly, whereas strs and stnet implemented Pohar-Perme estimator into discrete time, where survival time is split into intervals (4).\nIn this tutorial, two different datasets were applied to compare the estimates of rs.surv in R, stns, stpp, strs, and stnet in Stata. The colon.dta was used as an example data of tied times (discrete survival times), whereas the scenario2_1.dta has untied times (continuous survival times).\nAim  To understand why different esimates exist between/within the commands as they treat tied/untied survival time data using the same Pohar-Perme estimator.  References  Zhang MJ. Tied Survival Times. Encyclopedia of Biostatistics. 2005. doi: https://doi.org/10.1002/0470011815.b2a11075 Dickman P, Coviello E. Estimating and modelling relative survival. The Stata Journal. 2015;15:186-215. Pohar Perme M, Stare J, Estève J. On Estimation in Relative Survival. Biometrics. 2012;68:113-120 Comparison between stns and stpp, please refer to https://pclambert.net/software/stpp/  ","date":1600732800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1600732800,"objectID":"9077b01bd6bd51384ac0f2c6c9b8220c","permalink":"https://enochytchen.com/tutorials/relsurv/","publishdate":"2020-09-22T00:00:00Z","relpermalink":"/tutorials/relsurv/","section":"tutorials","summary":"rs.surv in R and stpp, strs, stnet in Stata are applied to estimate relative survival. What should we pay attention to as using tied/untied data? Why are the estimates different?","tags":null,"title":"Estimate relative survival $:$ tied data vs. untied data","type":"docs"},{"authors":null,"categories":null,"content":"Enoch Chen, Paul Dickman\nIntroduction An R ratetable object contains event rates per unit time and is typically used to estimate expected event rates for analyses in epidemiology where one compares the numbers of observed and expected events (e.g., SIR, SMR, or relative survival). Our interest lies in using rate tables for estimating relative survival (the ratio of observed survival among a cohort of cancer patients to the expected survival in the absence of cancer). The ratetable in R is analogous to what is often called the \u0026lsquo;popmort file\u0026rsquo; used in relative survival in Stata (e.g., by the strs command).\nFor cancer patient survival, ratetables are typically stratified by age, sex, and year and based on rates tabulated by government statistics offices. Mortality rates for many countries are available at the Human Mortality Database and can be converted to a ratetable object using the relsurv::transrate.hmd() function. Occasionally we wish to stratify the ratetable by additional factors (e.g., race, social class, or comorbidity) for which tabulated rates are not available. This tutorial illustrates how to create a ratetable from individual data for a cohort that is representative of the target population (i.e., a random sample of individuals without a diagnosis of cancer). For example, we may wish to stratify the expected mortality rates by socio-economic status, in addition to age, sex, and year. Such rates are often not available, but we may have access to a cohort representative of the general population (called the \u0026lsquo;control population\u0026rsquo;) from which we can construct such rates. That is, we assume we have access to a cohort representative of the target population (i.e., a random sample of individuals without a diagnosis of cancer) with follow-up information to facilitate estimation of all-cause mortality.\nThere are other approaches of generating expected survival from a sub-population (Bower et al. 2018, Rochet et al. 2015). However, the approach we describe here is not as sofisticated; we will simply model the control population to produce the ratetable stratified by the desired factors. The approach is straightforward; fit a suitable model for all-cause mortality and predict rates for each combination of variables one wants in the ratetable. We will fit a Royston-Parmar model (a.k.a.\\ flexible parametric model). We need to model two time scales; we\u0026rsquo;ll use age as the primary timescale (which is modelled as a natural spline as it\u0026rsquo;s the baseline hazard) and calendar year as the second time scale (by time-splitting and explicitly fitting a natural spline).\nWe are not aware of suitable publicly-available individual-level data from the general population, so we will use data on a cohort of patients with a cancer diagnosis as an illustrative example. The data are available (in Stata format) at https://pauldickman.com/data/colon.dta. We\u0026rsquo;ll model the data as a function of age, sex, and year. We don\u0026rsquo;t have a sensible \u0026lsquo;extra\u0026rsquo; dimension (e.g., education or social class) but will use subsite and stage as the additional dimensions. That is, this data set is used purely to demonstrate the approach and the R code. In practice, one would apply the approach to a cohort of individuals representative of the general population.\nSteps in a nutshell  Model all-cause mortality using a flexible parametric model with two time scales; attained age is modeled continuously (as te underlying timescale) and attained calendar year is a time-split variable. Save the predicted rates for each combination of variables in a data frame. In this example, besides sex, age, and calendar year, we use other two additonal dimensions (subsite + stage). Split the data frame into lists in order to apply transrate() and joinrate() to create a ratetable object. The relsurv::transrate() function converts a data frame containing rates into a ratetable object. The function only operates on data frames containing rates stratified by sex, age, and year (i.e., it does not support additional dimensions). To add aditional dimensions we need to apply transrate() to rates for each level of the additional dimensions and then join the resulting ratetables using joinrate(). As a test, use rs.surv()  along with the newly created ratetable to estimate net survival. The resulting estimates of net survival should be 1.  References Bower H, Andersson TM, Crowther MJ, Dickman PW, Lambe M, Lambert PC. Adjusting Expected Mortality Rates Using Information From a Control Population: An Example Using Socioeconomic Status. Am J Epidemiol. 2018;187(4):828-836. doi: 10.1093/aje/kwx303.\nRachet B, Maringe C, Woods LM, Ellis L, Spika D, Allemani C. Multivariable flexible modelling for estimating complete, smoothed life tables for sub-national populations. BMC Public Health. 2015;15:1240. doi: 10.1186/s12889-015-2534-3.\nAcknowledgements We especially want to thank Alexander Ploner for providing statistical consulting and R code optimisation tips, Nurgul Batyrbekova for her insight in modeling survival data on multiple time scale, Joshua Entrop for his teaching on plotting regression model results, and Quang Thinh Trac for his assistance in optimising the syntax.\n","date":1597795200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1598140800,"objectID":"b5f9bd09aab26f10b26aafe7be7e9c66","permalink":"https://enochytchen.com/tutorials/ratetable/","publishdate":"2020-08-19T00:00:00Z","relpermalink":"/tutorials/ratetable/","section":"tutorials","summary":"An alternative approach to constructing a ratetable object in R by fitting a multiple time-scale flexible parametric models using individual-level data","tags":null,"title":"Make ratetable using R","type":"docs"},{"authors":null,"categories":null,"content":"Learning outcomes In this section, you will learn how to\n ensure the analysis is reproducible work coherently and efficiently with yourself ensure the project can be understood by others (supervisors, collaborators, and future readers) create a good work flow and enhance accuracy of work  Course notes Slides can be downloaded (in PDF format) here.\nStudy notes No study notes in this section\nExcercise ","date":1601424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601424000,"objectID":"270487c791f84e8997ddb54182815cd5","permalink":"https://enochytchen.com/courses/biostatbasics/dataman/","publishdate":"2020-09-30T00:00:00Z","relpermalink":"/courses/biostatbasics/dataman/","section":"courses","summary":"Learning outcomes In this section, you will learn how to\n ensure the analysis is reproducible work coherently and efficiently with yourself ensure the project can be understood by others (supervisors, collaborators, and future readers) create a good work flow and enhance accuracy of work  Course notes Slides can be downloaded (in PDF format) here.","tags":null,"title":"Data management","type":"book"},{"authors":null,"categories":null,"content":"Learning outcomes In this section, you will learn how to Video lectures   Course notes Slides can be downloaded (in pdf format) here.\nStudy notes No study notes for this session\nStata syntax in the slides Stata codes be downloaded (do file) here.\n","date":1601424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601424000,"objectID":"d3363a12a1899fc4c86254a52527786d","permalink":"https://enochytchen.com/courses/biostatbasics/dataclearance/","publishdate":"2020-09-30T00:00:00Z","relpermalink":"/courses/biostatbasics/dataclearance/","section":"courses","summary":"Learning outcomes In this section, you will learn how to Video lectures   Course notes Slides can be downloaded (in pdf format) here.\nStudy notes No study notes for this session","tags":null,"title":"Data Clearance","type":"book"},{"authors":null,"categories":null,"content":"Learning outcomes In this section, you will learn how to\n get to know the data by performing summarize, describe, codebook, list measure central tendency of the data (mean, median, amd mode) measure dispersion of the data (range, interquartile range, variance, and standard deviation) make a two-by-two table ad interpret relative risk and odds ratio from it by using csi in epitab package  Video lectures   Course notes Slides can be downloaded (in pdf format) here.\nStudy notes  Ch1 Descriptive statistics\nStata syntax in the slides Stata codes be downloaded (do file) here.\n","date":1601424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601424000,"objectID":"6bbda1a7df9a2640d7d3d2733410ae2d","permalink":"https://enochytchen.com/courses/biostatbasics/sumstat/","publishdate":"2020-09-30T00:00:00Z","relpermalink":"/courses/biostatbasics/sumstat/","section":"courses","summary":"Learning outcomes In this section, you will learn how to\n get to know the data by performing summarize, describe, codebook, list measure central tendency of the data (mean, median, amd mode) measure dispersion of the data (range, interquartile range, variance, and standard deviation) make a two-by-two table ad interpret relative risk and odds ratio from it by using csi in epitab package  Video lectures   Course notes Slides can be downloaded (in pdf format) here.","tags":null,"title":"Summary Statistics","type":"book"},{"authors":null,"categories":null,"content":"Learning outcomes In this section, you will learn how to\n use Stata to create histograms, bar charts, box plots, scatter plots, and line graphs edit and customise graphs in twoway export graph in any format using graph export  Video lectures Course notes Slides can be downloaded (in pdf format) here.\nStudy notes Please refer to 1.3 Data presentation in Ch1 Descriptive statistics\nStata syntax shown in the lecture Stata codes be downloaded (do file) here.\n","date":1600905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600905600,"objectID":"cc8f5cfd3f26f4ce0ba2ede1cc23da15","permalink":"https://enochytchen.com/courses/biostatbasics/graphs/","publishdate":"2020-09-24T00:00:00Z","relpermalink":"/courses/biostatbasics/graphs/","section":"courses","summary":"Learning outcomes In this section, you will learn how to\n use Stata to create histograms, bar charts, box plots, scatter plots, and line graphs edit and customise graphs in twoway export graph in any format using graph export  Video lectures Course notes Slides can be downloaded (in pdf format) here.","tags":null,"title":"Graphs","type":"book"},{"authors":null,"categories":null,"content":"This is a testing page for using Stata Markdown. The questions and solutions are corresponding to their stmd files.\n   Description Markstat Questions Markstat Solutions Stata     Cox regression with observed (all-cause) mortality as the outcome Q122  Q122 S122  S122 s122.do   Calculating excess and ‘avoidable’ deaths from life tables  Q282  Q282 S282  S282 s282.do    ","date":1603670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603670400,"objectID":"df8dbf199b3c07360975b1c4b2cfbde9","permalink":"https://enochytchen.com/cansurv/exercises/","publishdate":"2020-10-26T00:00:00Z","relpermalink":"/cansurv/exercises/","section":"cansurv","summary":"This is a testing page for using Stata Markdown. The questions and solutions are corresponding to their stmd files.\n   Description Markstat Questions Markstat Solutions Stata     Cox regression with observed (all-cause) mortality as the outcome Q122  Q122 S122  S122 s122.","tags":null,"title":"Exercises","type":"book"},{"authors":null,"categories":null,"content":"Learning outcomes Study notes Study cite goldman and pagemo\nCourse notes Slides can be downloaded (in PDF format) here.\nStata syntax shown in the lecture Stata:\n","date":1601424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601424000,"objectID":"b720d013d2d59e62b386a43a993e5c37","permalink":"https://enochytchen.com/courses/biostatbasics/confidence/","publishdate":"2020-09-30T00:00:00Z","relpermalink":"/courses/biostatbasics/confidence/","section":"courses","summary":"Learning outcomes Study notes Study cite goldman and pagemo\nCourse notes Slides can be downloaded (in PDF format) here.\nStata syntax shown in the lecture Stata:","tags":null,"title":"Confidence Intervals","type":"book"},{"authors":null,"categories":null,"content":"Learning outcomes Study notes Study cite goldman and pagemo\nCourse notes Slides can be downloaded (in PDF format) here.\nStata syntax shown in the lecture Stata:\n","date":1601424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601424000,"objectID":"982ba2296d781629ec3767dd484e8a74","permalink":"https://enochytchen.com/courses/biostatbasics/hypotest/","publishdate":"2020-09-30T00:00:00Z","relpermalink":"/courses/biostatbasics/hypotest/","section":"courses","summary":"Learning outcomes Study notes Study cite goldman and pagemo\nCourse notes Slides can be downloaded (in PDF format) here.\nStata syntax shown in the lecture Stata:","tags":null,"title":"Hypothesis tests","type":"book"},{"authors":null,"categories":null,"content":"(Not completed yet)\nThe codes used in this tutorial are available below.\nrs.surv\nstns, stpp, strs, stnet\nThis example showed how the relative survival estimates change given more and more ties are added into the data.\nDataset The dataset (scenario2_1.dta) is a simulated data, containing 1000 subjects. The original survival time is untied data with 651 distinct values in years (min: 0.0027; max: 12). We then added ties into the data by flooring the time into discrete days, weeks, months, quaters, or years. More ties are added (i.e., less distinct values) as the time interval becomes larger (days→years). If any survival time became 0 due to flooring, we then made it 0.5 of the time interval (e.g., 0.5 months).\nThe following table shows how the time was treated in different units.\n   Time variable Discrete time in Stata code Distinct values     tt1 Original continuous time  651   tt2 Days (floor(tt1*365.241) + 0.5) /365.241 569   tt3 Weeks (floor(tt1*52.177) + 0.5) /52.177 332   tt4 Months (floor(tt1*12) + 0.5) /12 132   tt5 Quarters (floor(tt1*4) + 0.5) /4 49   tt6 Years (floor(tt1) + 0.5) 13    Relative survival estimates The following tables show the estimates of 1-, 5-, and 10-year relative survival (RS) using the Pohar-Perme estimator by rs.surv, stpp, strs, and stnet. To make the tables look tidier, here we dismissed the 95% CI, which however can be found in the outputs of the syntax.\nrs.surv()    Time  (year) tt1 original tt2  days tt3  weeks tt4  months tt5  quarters tt6  years     1 0.797 0.797 0.797 0.790 0.796 0.788   5 0.618 0.618 0.618 0.618 0.617 0.613   10 0.534 0.534 0.534 0.536 0.535 0.529    stns    Time  (year) tt1 original tt2  days tt3  weeks tt4  months tt5  quarters tt6  years     1 0.797 0.797 0.796 0.790 0.777 0.711   5 0.613 0.613 0.613 0.614 0.604 0.600   10 0.512 0.512 0.512 0.514 0.513 0.510    stpp    Time  (year) tt1 original tt2  days tt3  weeks tt4  months tt5  quarters tt6  years     1 0.797 0.797 0.796 0.790 0.777 0.709   5 0.613 0.613 0.613 0.613 0.603 0.595   10 0.512 0.512 0.512 0.509 0.502 0.484    strs    Time  (year) tt1 original tt2  days tt3  weeks tt4  months tt5  quarters tt6  years     1 0.798 0.797 0.795 0.792 0.778 0.732   5 0.612 0.612 0.612 0.616 0.605 0.617   10 0.513 0.513 0.513 0.512 0.505 0.506    stnet    Time  (year) tt1 original tt2  days tt3  weeks tt4  months tt5  quarters tt6  years     1 0.798 0.797 0.795 0.798 0.798 0.799   5 0.612 0.612 0.612 0.612 0.613 0.614   10 0.513 0.513 0.513 0.513 0.513 0.514    Explanation  Generally, introducing ties into the data did not change the esitmates of relative survival no matter which package was used for calculation in this case. One exception happened if discrete time was made into years (tt6), which merely 13 distinct values exist, rs.surv() and strs gave slightly lower estimates for 1-year relative survival (0.788 and 0.732 separately); stns and stpp gave much lower survival, particularly in 1-year net survival; surprisingly, the output estimated by stnetalmost did not change given heavy ties were added. One thing we should bear in mind is that introducing ties to the data has in fact changed the original data. I tried to add ties in a sensible way and not to change the order of the data as much as possible. However, I am innocent of the lower or higher estimates rs.surv(), stns, stpp, and strs gave.  Conclusion  It is common that the data from cancer register has tied time, e.g., due to protection on patients\u0026rsquo; privacy. As estimating survival, we should pay attention to the potential change on the estimates given the amount of tie is introduced.  ","date":1602115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602115200,"objectID":"30198c56169a0cc60cb3b43b9980eb1a","permalink":"https://enochytchen.com/tutorials/relsurv/untied/","publishdate":"2020-10-08T00:00:00Z","relpermalink":"/tutorials/relsurv/untied/","section":"tutorials","summary":"(Not completed yet)\nThe codes used in this tutorial are available below.\nrs.surv\nstns, stpp, strs, stnet\nThis example showed how the relative survival estimates change given more and more ties are added into the data.","tags":null,"title":"Untied time data → Tied","type":"docs"},{"authors":null,"categories":null,"content":"Title Cancer survival statistics for patients and healthcare professionals–a tutorial of real‐world data analysis\nAuthors Eloranta S, Smedby KE, Dickman PW, Andersson TM\nNotes  Why do we use net survival?\nWe want to compare survival between groups and over time. Definition of net survival: In a hypothetical world, cancer patients can only die from the cancer of interest. Comparison between   Cause-specific survival:\na. requires information on cause-specific deaths; deaths due to cancer/other than caner\nb. cause-specific death is not always complete or available. Relative survival:\na. does not require information on cause-specific deaths\nb. requires a reference population, e.g., life-table data\nc. the mortality analogue of relative survival is excess mortality, which captures both direct and indirect mortality due to cancer.  Competeing events\nNet survival measure is a clever way to compare survival between different groups, but in the real world, competing events exist. Loss of life expectancy: the number of life years a cancer patient is estimated to lose.  Reference Eloranta S, Smedby KE, Dickman PW, Andersson TM. Cancer survival statistics for patients and healthcare professionals–a tutorial of real‐world data analysis. Journal of Internal Medicine. 2020. doi: https://doi.org/10.1111/joim.13139\n","date":1601769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601769600,"objectID":"92a17e2fc31de5f0c129b4f233b60434","permalink":"https://enochytchen.com/tutorials/learningrs/eloranta2020/","publishdate":"2020-10-04T00:00:00Z","relpermalink":"/tutorials/learningrs/eloranta2020/","section":"tutorials","summary":"Title Cancer survival statistics for patients and healthcare professionals–a tutorial of real‐world data analysis\nAuthors Eloranta S, Smedby KE, Dickman PW, Andersson TM\nNotes  Why do we use net survival?","tags":null,"title":"Eloranta2020","type":"docs"},{"authors":["Enoch Chen"],"categories":[],"content":"Table of Contents    Basics of survival analysis Proof of relative survival References     It was mentioned numerous times in different articles that excess hazard is the analogue of relative survival on the hazard scale (1-4). However, how is it derived from?\nWe first look into the basic definition of relative survival and excess hazard. Relative survival, $R(t)$, is defined as the ratio between the survival of the cancer patient cohort, $S(t)$, and the reference population, $S^*(t)$, whereas excess hazard, $\\lambda (t)$, is the all-cause hazard, $h(t)$, minus the expected hazard, $h^*(t)$. \\begin{equation} R(t) = \\frac{S(t)}{S^*(t)} \\end{equation} \\begin{equation} \\lambda (t) = h(t) - h^*(t) \\end{equation}\nBasics of survival analysis Survival function To illustrate the relationship in between these two terms, we need to come back to some basics of hazard and survival (5). First, in almost every chapter one of any survival analysis book, you can find the definition of cumulative distribution function (c.d.f.) of a time $T$ variable, giving the probablity that the event has occurred as time is less than $t$.\n\\begin{equation} F(t)=\\operatorname{P}(T\u0026lt;t) \\end{equation}\nThe complement of this c.d.f. is survival function, , which is a proportion, shown as \\begin{equation} S(t)=\\operatorname{P}(T \\geq t)=1-F(t)=\\int_{t}^{\\infty} f(x) d x, \\end{equation} which is defined as the probability that the event has not occurred by time $t$. That is, for example, the probability that the patient is still alive by time $t$, where $f(t)$ is the probability density function (p.d.f.). And $-f(t)$ is the derivative of $S(t)$. $$\\begin{eqnarray} \\frac{d S(t)}{dt} \u0026amp;=\u0026amp; \\frac{d}{dt} \\int_{t}^{\\infty} f(x) d x \\\\\\\n\u0026amp;=\u0026amp; -f(t) \\end{eqnarray}$$\nHazard function We have to know by heart that the hazard function is the instantaneous rate of occurrence of the event, which is a rate (event occurrence per unit of time).\n\\begin{equation} h(t)=\\lim _{d t \\rightarrow 0} \\frac{\\operatorname{P}(t \\leq T\u0026lt;t+d t \\mid T \\geq t)}{d t} \\end{equation}\nLet us take a look at the numerator itself, which is a conditional probability. \\begin{equation} \\operatorname{P}(t \\leq T\u0026lt;t+d t \\mid T \\geq t) = \\frac{P(t \\leq T\u0026lt;t+dt )}{P(T\\geq t)}, \\end{equation} where the denominator part is in fact the survival function, $S(t)$, and the numerator part means the probability that the event happens $[t, t+dt)$, which is $f(t)dt$. So we then extend the equation: $$\\begin{eqnarray} \\operatorname{P}(t \\leq T\u0026lt;t+d t \\mid T \\geq t) \u0026amp;=\u0026amp; \\frac{P(t \\leq T\u0026lt;t+dt )}{P(T\\geq t)}\n\\\\\\ \u0026amp;=\u0026amp;\\frac{f(t)dt}{S(t)} \\end{eqnarray}$$ Dividing both sides by $dt$, it gives the relationship between the hazard function and the survival function, which is $$\\begin{eqnarray} h(t) \u0026amp;=\u0026amp; \\frac{f(t)}{S(t)} \\\\\\\n\u0026amp;=\u0026amp; \\frac{\\frac{-d S(t)}{dt}}{S(t)} \\\\\\\n\u0026amp;=\u0026amp; \\frac{-{S}'(t)}{S(t)} \u0026amp;=\u0026amp; \\frac{-d \\ln S(t)}{dt} \\end{eqnarray} $$ (Please catch up your Calculus 101: $\\frac{d \\ln x}{dx} = \\frac{1}{x}$)\nIf we rewrite the equation above, we will obtain: \\begin{equation} \\ln S(t) = -\\int_{0}^{t} h(x) d x \\end{equation}\n$$\\begin{eqnarray} S(t) \u0026amp;=\u0026amp; exp(-\\int_{0}^{t} h(x) d x) \\\\\\\n\u0026amp;=\u0026amp; exp(-H(t)) \\end{eqnarray}$$\nThis is pretty! Then we get to know the cumulative hazard function.\nCumulative hazard function The cumulative hazard function is the integral from 0 to time $t$, defined as \\begin{equation} H(t)=\\int_{0}^{t} h(x) d x. \\end{equation} Thus, it is easy to understand that the hazard function is the derivative of the cumulative hazard function. \\begin{equation} h(t) = \\frac{d H(t)}{dt} \\end{equation}\nProof of relative survival \\begin{equation} R(t) = \\frac{S(t)}{S^*(t)} \\end{equation} First, by defintion of the relationship between the cumulative hazard function and the survival function, we will obtain \\begin{equation} exp(- \\Lambda (t)) = \\frac{exp(- H(t))}{exp(-H^*(t))} \\\\\\\n\\end{equation} \\begin{equation} exp(- \\Lambda (t)) = exp(- H(t) + H^*(t)) \\end{equation} \\begin{equation} \\Lambda (t) = H(t) - H^*(t) \\end{equation}\nThen we got the cumulative excess hazard, $\\Lambda (t)$, is the difference between the cumulative all-cause hazard (of the cancer patient cohort) and the cumulative expected hazard (of the reference population). Then, take the derivative of both sides, shown as:\n\\begin{equation} \\frac{d \\int_{0}^{t} \\lambda(x) d x }{dt} = \\frac{d \\int_{0}^{t} h(x) d x }{dt} - \\frac{d \\int_{0}^{t} h^*(x) d x }{dt}, \\end{equation} which is exactly,\n\\begin{equation} \\lambda (t) = h(t) - h^*(t) \\end{equation}\nThis is fantastic! Now you know why excess hazard is the hazard analogue of relative survival.\nReferences  Andersson TML, Dickman PW, Eloranta S, Lambe M, Lambert PC. Estimating the loss in expectation of life due to cancer using flexible parametric survival models. Statistics in Medicine 2013;32:5286–5300. Andersson T. Quantifying cancer patient survival: extensions and applications of cure models and life expectancy estimation. Phd thesis, Karolinska Institutet, 2013. Eloranta S. Development and Application of Statistical Methods for Population-Based Cancer Patient Survival. Phd thesis, Karolinska Institutet, 2013. Eloranta S, Smedby KE, Dickman PW, Andersson TM. Cancer survival statistics for patients and healthcare professionals–a tutorial of real‐world data analysis. Journal of Internal Medicine. 2020. Germán Rodríguez. Generalized Linear Models: Survival Models. https://data.princeton.edu  ","date":1601721965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601721965,"objectID":"76b0aa1c824d3737956e39bd391dfa28","permalink":"https://enochytchen.com/post/analogue/","publishdate":"2020-10-03T12:46:05+02:00","relpermalink":"/post/analogue/","section":"post","summary":"Table of Contents    Basics of survival analysis Proof of relative survival References     It was mentioned numerous times in different articles that excess hazard is the analogue of relative survival on the hazard scale (1-4).","tags":[],"title":"Excess hazard is an analogue of relative survival. How come!?","type":"post"},{"authors":null,"categories":null,"content":"The codes used in this tutorial are available below.\nrs.surv\nstns, stpp, strs, stnet (original tied data)\nstns, stpp, strs, stnet (untied data)\nSettings This tutorial first used the colon.dta, which is originally a tied survival time data, to generate the relative survival estimates of 1-, 5-, and 10-year relative survival using the Pohar-Perme estimator (1), by rs.surv() in R, stpp, strs, stnet in Stata. In the second part, an small epislon ~ $Norm(0, 0.01)$ was added into each individual\u0026rsquo;s survival time to make no individual have exactly the same survival time (to untie the data).\n Survival time was calculated by (date of exit - date of diagnosis). We define failure as status == 1 2  (died from cancer or other causes). In strs, ht (hazard transformation) must be specified to command the survival be calculated by transforming cumulative excess hazard ($\\lambda$) instead of an actuarial approach (default) (2). However, in stnet, by default, survival is calculated by using hazard transformation. In strs and stnet, monthly intervals were calculated up to ten years by specifying br(0(`=1/12')10).  Results To make the tables look tidier, here we dismissed the 95% CI, which however can be found in the outputs of the syntax. The esimates of 1-, 5-, and 10-year relative survival by each program are shown below:\nUsing colon.dta (original tied time data)    t rs.surv stns stpp strs stnet     1 0.676 0.677 0.676 0.677 0.677   5 0.473 0.474 0.473 0.474 0.474   10 0.433 0.437 0.434 0.434 0.434    Using colon.dta (untied time data)    t rs.surv stns stpp strs stnet     1 0.676 0.677 0.677 0.677 0.677   5 0.472 0.474 0.473 0.474 0.474   10 0.431 0.435 0.433 0.434 0.434    Explanation  By default, both rs.surv(), stns, stpp calculate survival using the product integral method on the hazard level, whereas in strs and stnet time-scale is split into numbers of intervals (using actuarial life-table approach). In stpp, the Fleming-Harrington estimator (using the expoential of the negative cumulative (excess) hazard), which appears to be more sensitive to ties, is also eligible to be applied (3). stnet should generate the identical estimates as strs, given that ht is specified in strs. Removing ties did not have an effect with discrete time estimators using life-table framework. (strs and stnet). In strs and stnet, life-table framework is implemented to estimate relative survival. However, should the cutpoints be in months br(0(`=1/12')10) or in years br(0(1)10)?\nA: Monthly estimate is more accurate. Both strs and stnet - calculates the attained age and attained year at the beginning of each interval and takes the floor() of these values from the popmort file to obtain the expected mortality rate. However, typically the survivaly probility from the popmort file (calculated from 1-year probability of death, by using by $-exp(H)$) is the probability of surviving 1 year, $p$. If it is monthly interval, we take the twelth root of the survival probability, $p^{1/12}$. Calculating by month literally means we do it 12 times to calculate the survival from an $x$ year-old person until he turns $(x+1)$ years old, but if using annual interval, we do it once instead.   It is worth paying attention to how age and year are managed in each program.\n stns: age of dianosis in the cohort data and age in the popmort file are needed in age(); year of dianosis date of diagnosis and the calendar year in the popmort file should be specified in period() . stpp: age and date of diagnosis should be specified in agediag() and datediag(). strs: age and year of diagnosis are required in diagage() and diagyear(). stnet: date of dianosis diagdate() and date of birth birthdate() are required.    References  Pohar Perme M, Stare J, Estève J. On Estimation in Relative Survival. Biometrics. 2012;68:113-120 Dickman P, Coviello E. Estimating and modelling relative survival. The Stata Journal. 2015;15:186-215. Fleming TR, and Harrington DP. Nonparametric Estimation of the Survival Distribution in Censored Data.Communications in Statistics—Theory and Methods. 1984;13:2469–2486.  Acknowledgement We especially want to say thank-you to Paul Lambert for offering his scenario2_1.dta as an example dataset and substantial insights on the syntaxes of stnsand stnet.\n","date":1601251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601251200,"objectID":"01e3d96db9ac1e122778e68d7736b846","permalink":"https://enochytchen.com/tutorials/relsurv/tied/","publishdate":"2020-09-28T00:00:00Z","relpermalink":"/tutorials/relsurv/tied/","section":"tutorials","summary":"The codes used in this tutorial are available below.\nrs.surv\nstns, stpp, strs, stnet (original tied data)\nstns, stpp, strs, stnet (untied data)\nSettings This tutorial first used the colon.dta, which is originally a tied survival time data, to generate the relative survival estimates of 1-, 5-, and 10-year relative survival using the Pohar-Perme estimator (1), by rs.","tags":null,"title":"Tied time data → Untied time data","type":"docs"},{"authors":null,"categories":null,"content":"In the last section of this tutorial, we would like to briefly introduce two other approaches for making rate table from a sub-population (Bower et al. 2018, Rachet et al. 2015). First, to summarise these three approaches, we made a comparison chart shown below:\n    Our approach Bower 2018 Rachet 2015     Nationwide life table x v Flexible relation model: v Flexible Poisson model: x   Individual data on a reference population v v individual-level or grouped data   Dealing with unceratinty x Parametric bootstrapping Using data on multiple sampling populations   Characteristics Modelling mortality rates directly without requiring nationwide life table Stabilised by nationwide life table Alternative to avoiding integrating the pattern of nationwide life table, but still obtaining robust estimates    Example 1: Adjust expected mortality rate by using adjustment factor  Bower et al. (2018) used the individual-level data from a reference population to scale the expected mortality for each level of social class, aided by nationwide life tables.\nRather than using the the typical defintion of relative survival ($R(t)$, defined as the ration between $S(t)$, the all-cause survival of a patient cohort and $S^*(t)$, the expected survival of a general population), Bower et al. extended it to estimate the relative survival between the breast cancer patients and the Swedish population, with available data on SES, age at diagnosis, year of diagnosis (in their case, all the breast cancer patients were women, so sex was exempted), shown below:\n$$ R\\left(t \\mid \\mathrm{SES}, a_{d}, y_{d}\\right)=\\frac{S\\left(t \\mid \\mathrm{SES}, a_{d}, y_{d}\\right)}{S^{*}\\left(t \\mid \\mathrm{SES}, a_{d}, y_{d}\\right)} $$\nThat is, on hazard scale, the expected hazard (or so-called expected mortality rate) is required to be stratified by SES. Thus, they proposed estimating adjustment factors, $\\rho_{j}$, corresponding to SES grops, and multiplied it with the unadjusted rate to generate SES-adjusted mortality rate. $$ \\frac{h_{c}(a, y, j)}{h^{*}(a, y)}=\\exp \\left[\\sum_{j=1}^{3} \\rho_{j}(a, y) \\times \\operatorname{SES}_{j}\\right] $$\nIn the article, they further showed that how this framework works using Poisson models and flexible parametric models with restricted cubic spline function. For tackling uncertainty in using a sub-population, they ran a parametric bootstrap method for 100 times.\nExample 2: Multivariable flexible modelling  Rachet et al. (2015) proposed two alternative approaches to construct smoothed life tables for sub-populations. Both these two models can be applied into either abriged or complete data. That is to say, both individual-level data and grouped data can be modelled.\nFlexible relational model The first one, flexible relational model, requires survival function of a reliable standard population, $l_{x_{s}}$, and multivariables, such as deprivation level and the interaction between deprivation level and age.\n$$ \\begin{aligned} \\operatorname{logit}\\left(l_{x, i}\\right) \u0026amp;=f\\left(\\log i t\\left(l_{x_{s}}\\right)\\right) +\\sum_{i=2}^{5} \\beta_{i} \\text {dep}_{i}+g(\\text {agedep}) \\end{aligned}, $$ where $f$ and $g$ are restricted cubic spline functions.\nFlexible Poisson model The second one, flexible Poisson model, does not require external infromation on standard population mortality, and it models age-specific death counts with multiple variables. The advantage is that this approach takes account of the variance existing in the data.\n$$ \\begin{aligned} \\log \\left(d_{x, i}\\right) \u0026amp;=\\beta_{0}+f(x)+\\sum_{i=2}^{5} \\beta_{i} \\text {dep}_{i}+g(\\text {agedep})+\\log \\left(\\text {pyrs}_{x, i}\\right) \\end{aligned}, $$\nConclusion Our approach is distinct from Bower et al. and Rachet et al. , since only individual-level data is required to model the mortality rate directly from a representative population free of the disease of interest, the advantages of which are that in some situation nationwide life tables are not available and allowing more variance on the estimates directly generated from the reference population. However, the disadvantage is that the estimates of the model is not aided by the underlying pattern of existing life tables.\nIn terms of concerns on uncertainty, Bower et al. used a sensible approach of bootstrapping to tackle the issue, which is eligible to be considered in optimising our approach.\nReferences Bower H, Andersson TM, Crowther MJ, Dickman PW, Lambe M, Lambert PC. Adjusting Expected Mortality Rates Using Information From a Control Population: An Example Using Socioeconomic Status. Am J Epidemiol. 2018;187(4):828-836. doi: 10.1093/aje/kwx303.\nRachet B, Maringe C, Woods LM, Ellis L, Spika D, Allemani C. Multivariable flexible modelling for estimating complete, smoothed life tables for sub-national populations. BMC Public Health. 2015;15:1240. doi: 10.1186/s12889-015-2534-3.\n","date":1600732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600732800,"objectID":"a2b3cfe853452fdc54ce35071b363c87","permalink":"https://enochytchen.com/tutorials/ratetable/otherapp/","publishdate":"2020-09-22T00:00:00Z","relpermalink":"/tutorials/ratetable/otherapp/","section":"tutorials","summary":"In the last section of this tutorial, we would like to briefly introduce two other approaches for making rate table from a sub-population (Bower et al. 2018, Rachet et al. 2015).","tags":null,"title":"Make ratetable using R","type":"docs"},{"authors":["Enoch Chen"],"categories":null,"content":"","date":1598873400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598873400,"objectID":"d8a12b59a88bb0e27e3b786689245e74","permalink":"https://enochytchen.com/talk/mphthesis/","publishdate":"2020-08-26T00:00:00Z","relpermalink":"/talk/mphthesis/","section":"talk","summary":"Talk about my thesis journey to freshers to the Master's Programme in Public Health Sciences at Karolinska Institutet","tags":[],"title":"Thesis Journey at MPH, Epidemiology","type":"talk"},{"authors":["Enoch"],"categories":[],"content":"Table of Contents    Introduction Principles Conclusion Reference     Introduction My recent work was on analyzing a patient cohort data. In the syntax documentation, there was one sentence I wrote, \u0026ldquo;We then exacted the birth date of each observation.\u0026rdquo; My colleague then suggested me to use the other term \u0026ldquo;each individual\u0026rdquo; instead. The reason behind his argument is to avoid dehumanizing language. This is such a important component of scientific writing I should have picked up!\nIn scientific writing, we aim to pre-conceive ideas about individuals, both the treatment group and the control group, whereas it is essential to use correct languages to describe them. Incorrect, or even poorly selected, words can harm patients. Furthermore, it definitely has negative impact on the relationship between clinicians/researchers and patients. Once trust is broken, it is difficult to regain.\nLeopold S. et al wrote an article, entitled: Editorial: Words Hurt – Avoiding Dehumanizing Language in Orthopaedic Research and Practice (1), offers a holistic viewpoint on using people-first language on scientific writing, especially in biomedicine fields. Here are some priciples I summarized from their publication.\nPrinciples  Patients are humans. They should be referred to the correct relative pronoun \u0026ldquo;who\u0026rdquo; rather than \u0026ldquo;that\u0026rdquo;.   Patients that who present for treatment of \u0026hellip;  Make clear that patients are more than their symptoms and diagnoses.   Diabetic patients Patients with diabetes Amputees Patients who have had amputations  Evaluate patients\u0026rsquo; self perception. Thus, use correct terms to characterize control groups as well. A value-neutral terminology is preferred. Otherwise, we might in fact implicate insulting the treatment/exposure group.   Using normal/able-bodied individuals to refer to the control individuals. Are we saying that the treatment group is NOT normal/able-bodied.  Conclusion Being a thoughtful and sensible writer is the milestone of delievering good research language, isn\u0026rsquo;t it?\nReference  Leopold s et al, Editorial: Words Hurt – Avoiding Dehumanizing Language in Orthopaedic Research and Practice. Clin Orthop Relat Res. 2014 Sep; 472(9): 2561–2563. doi: 10.1007/s11999-014-3802-8  ","date":1598258953,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598258953,"objectID":"ebbb765e6d58ac7e764e13459394d288","permalink":"https://enochytchen.com/post/avoid_dehumanizing/","publishdate":"2020-08-24T10:49:13+02:00","relpermalink":"/post/avoid_dehumanizing/","section":"post","summary":"Does the term \"diabetic patients\" differ from \"patients with diabetes?\" Which term is more politically correct based on people-first language?","tags":[],"title":"Avoid dehumanizing language use","type":"post"},{"authors":null,"categories":null,"content":"Enoch Chen, Paul Dickman\nThe syntax used in this tutorial can be found here.\nData preparation Install required packages and read the data. We are using a data set containing information on individuals diagnosed with colon cancer (because the data are publically available). In a real application we would use data on individuals randomly selected from the general population.\n#' 1 Preparation #' Clear all #' Use it if you need to clear all #' rm(list = ls()) #' Load required packages x\u0026lt;-c( \u0026quot;haven\u0026quot;, # read.dta() \u0026quot;tidyverse\u0026quot;, # dplyr::mutate \u0026quot;lubridate\u0026quot;, # decimal_date \u0026quot;survival\u0026quot;, # Surv(), survSplit() \u0026quot;rstpm2\u0026quot;, # stpm2() \u0026quot;splines\u0026quot;, # nsx() \u0026quot;relsurv\u0026quot;, # transrate(), joinrate(), rs.surv() \u0026quot;popEpi\u0026quot;, # as.data.frame(ratetable) \u0026quot;ggplot2\u0026quot;) # ggplot() lapply(x, require, character.only = TRUE) #' Read the data from web colon \u0026lt;- read_dta( \u0026quot;http://enochytchen.com/directory/data/colon.dta\u0026quot;) str(colon)  We then created an approximate birth date for each individual by using dx (date of diagnosis) - age at diagnosis * 365.241. The unit here is days in order to be consistent with the unit in relsurv::rs.surv(). Variables sex, subsite, stage, and strata were converted into factor class, which will be automatically converted into dummy varaibles in a regression model in R. In this example, we used two dimensions: subsite + stage, and combined them into one stratum, a combination of all the dimensions except sex, age, and year. Splitting the data frame into lists will be done on this stratum afterwards. However, if there is only one extra dimension, it is not required to create a stratum. Instead, it is straightforward to split on that single variable.\n#' Approximate the date of birth, #' Will take care of derived variables (entry/exit years) later colon2 \u0026lt;- colon %\u0026gt;% mutate(sex = as_factor(sex), ## as_factor preserves labels status = as.numeric(status), subsite = as_factor(subsite), ## as_factor preserves labels stage = as_factor(stage), ## as_factor preserves labels strata = as_factor(paste(subsite, stage, sep = \u0026quot;, \u0026quot;)), # strata consists all the dimensions dob = as.Date(dx) - age*365.241, ) %\u0026gt;% select(id, sex, status, subsite, stage, strata, dob, dx, exit, age) str(colon2) summary(colon2)  Splitting the time We split calendar time into 2-year intervals; for our relatively small data set (15000 individuals) 1-year intervals resulted in convergence problems when we modelled mortality in a later step.\nIt is worth taking a look at the definition of episode in survival::survSplit, where it explains, \u0026ldquo;episode 1= less than the first cutpoint, 2= between the first and the second.\u0026rdquo; Based on this default setting, we then moved each episode (period) by 2.\n#' Split calendar time into 2-year intervals; #' splitting in 1-year intervals leads to convergence problems later if #' stpm2 does not have enough events within each time interval colon_split \u0026lt;- survSplit(Surv(decimal_date(dx), decimal_date(exit), status, type = \u0026quot;mstate\u0026quot;) ~ ., data = colon2, cut = seq(1975, 1995, by = 2), event = \u0026quot;status\u0026quot;, episode = \u0026quot;period\u0026quot; ) %\u0026gt;% # changed word \u0026quot;censor\u0026quot; to 0, so to keep it consistent with original definition mutate(status = as.numeric(ifelse(as.character(status) == \u0026quot;censor\u0026quot;, \u0026quot;0\u0026quot;, as.character(status))) ) #' Inspect: select the first 20 to take a look head(colon_split, 20) #' For downstream analysis, we want age as primary time scale; #' Calculate age at entry and age at exit #' Also, it would be nice to have the period expressed as actual starting year #' of the time interval; see ?survSplit for its definition #' (i.e. 1 = before first interval) colon_split = mutate(colon_split, age_start = tstart - decimal_date(dob), age_stop = tstop - decimal_date(dob), period = 1975 + (period - 2)*2) # *2 for 2-year intervals #' Save the data for running AIC/BIC test #' saveRDS(colon_split, \u0026quot;./Data/split_colon.rds\u0026quot;)  Flexible parametric model The colon cancer patients\u0026rsquo; data was modelled in multiple time scales, where the primary time scale is attained age calculated from two time points (age_start and age_stop), and the secondary is time-split calendar year fitted a natural spline using nsx() . Time-dependent effect of explanatory varibles was taken into consideration specifying tvc=list().\nfpm \u0026lt;- stpm2(Surv(time = age_start, time2 = age_stop, event = status==2) ~ sex + nsx(period, df=2) + subsite+ stage, data = colon_split, tvc = list(sex=3, period=2), df = 3) summary(fpm) #' A nice hazard plot #' Age-specific hazard by sex, subsite=2, stage =1, 1980 newdata = data.frame(sex = levels(colon2$sex), subsite = \u0026quot;Descending and sigmoid\u0026quot;, stage = \u0026quot;Localised\u0026quot;, period = 1980) newdata plot(fpm, newdata = newdata[2,,drop=FALSE], type = \u0026quot;hazard\u0026quot;, ci = FALSE, xlim = c(40,110), xlab = \u0026quot;Attained age (years)\u0026quot;, ylim = c(1E-6,100), ylab = \u0026quot;Hazard (log-scale)\u0026quot;, log = \u0026quot;y\u0026quot;, main = \u0026quot;Age-specific log-hazard for subsite=2, stage =1, period 1980, by sex\u0026quot;) lines(fpm, newdata = newdata[1,,drop=FALSE], type = \u0026quot;hazard\u0026quot;, lty = 2, col = \u0026quot;red\u0026quot;) legend(\u0026quot;topleft\u0026quot;, legend=c(\u0026quot;Female\u0026quot;, \u0026quot;Male\u0026quot;), col=c(\u0026quot;black\u0026quot;, \u0026quot;red\u0026quot;), lty=1:2)    Predicted rates in a data frame An empty data frame was generated using expand.grid() stratified by all the dimensions: sex, age_stop (age), period (year), subsite, and stage. The names of the variables must be consistent with which in the flexible parametric model. Then, predict() was applied to obtain predicted rates (i.e.,survival) in a data frame, similar to the concept in the extrapolation example using predict in Stata: Extrapolating survival (all-cause survival framework).\ncolon_new \u0026lt;- expand.grid(sex = levels(colon2$sex), subsite = levels(colon2$subsite), stage = levels(colon2$stage), age_stop = 1:110, period = 1975:1995) #'Populate the empty data frame with predicted hazards (based on the fitted model) colon_new$hazard \u0026lt;- predict(fpm, newdata = colon_new, type = \u0026quot;hazard\u0026quot;) colon_new \u0026lt;- colon_new %\u0026gt;% mutate(prob = exp(-hazard), age = age_stop, ) %\u0026gt;% select(sex, subsite, stage, age, period, prob, hazard) str(colon_new) #' Take look at the first 20 rows head(colon_new, 20)  Create ratetable Here comes the highlight of this tutorial!\nThe target focuses on how to transform the mortality rate dataframe into to a ratetable class data, which is required in rs.surv(). The subset of the mortality rate data frame, popmort_new, consists of age, sex, period, strata, and prob (survival probability).\nFirst, we splitted the data frame into lists by the varaible strata (subsite + stage in this example. i.e., all the combinations of the extra dimensions). Second, we splitted the lists by sex, so we are supposed to obtain the lists stratified by both strata and sex.\nspread() was then applied to transpose the matricies. Afterwards, transrate() combined the lists stratified by sex and joinrate() made the lists stratified by strata into a ratetable.\npopmort_new \u0026lt;- colon_new %\u0026gt;% mutate(strata = paste(subsite, stage, sep = \u0026quot;, \u0026quot;), )%\u0026gt;% select(age, sex, period, strata, prob) #' transrate() wants two matrices, both age x year, one for men, one for #' women; using split() repeatedly to make it work #' split() converted a data.fram into lists based on the specified variable #' First, split our popmort file by strata pm_split = split(popmort_new[, -4], popmort_new$strata) str(pm_split) #' Then we split the list again by sex pm_split = lapply(pm_split, function(x) split(x[, -2], x$sex)) str(pm_split) #' Using spread + as.matrix to generate the input matrices that transrate() #' needs spread_df = function(x) { ret = spread(x, period, prob) rownames(ret) = ret$age - 1 # Drop the age variable ret = ret[, -1] as.matrix( ret ) } pm_split = lapply(pm_split, function(x) lapply(x, spread_df )) str(pm_split) #' Now do the transrate() for each strar=ta; we get a list of #' ratetable-objects pm_split = lapply(pm_split, function(x) transrate(x$Male, x$Female, yearlim = c(1975, 1995))) str(pm_split) #' We can directly use the jointable-command on this list myratetable \u0026lt;- joinrate(pm_split, dim.name=\u0026quot;strata\u0026quot;) str(myratetable) #' Check whether is a readable ratetable for rs.surv() is.ratetable(myratetable)  Use rs.surv() to estimate net survival It is important to keep in mind that in relsuv::rs.surv() the unit of follow-up time is specified in days (1). Thus, given that the time in the original colon dataset is in years, we need to multiply the time by 365.241. The same rule applies to age, where in relsuv::rs.surv()::rmap age should be multiplied by 365.241 as well. The rate table was generated from this patient data. Therefore, if we estimate net survival using the same data, we should get relative survival close to 1, shown in the following syntax and graph.\nrssurv\u0026lt;-rs.surv(Surv(time = (decimal_date(exit)- decimal_date(dx)) * 365.241, event = status == 2) ~ sex + subsite+ stage, rmap = list(age = age*365.241, year = dx, strata = strata), ratetable = myratetable, data = colon2, method = \u0026quot;ederer2\u0026quot;) rssurv.sum \u0026lt;- summary(rssurv, time = c(0:10) * 365.241, scale = 365.241) rs.table \u0026lt;- as.data.frame(rssurv.sum[c(\u0026quot;strata\u0026quot;, \u0026quot;time\u0026quot;, \u0026quot;n.risk\u0026quot;, \u0026quot;n.event\u0026quot;, \u0026quot;surv\u0026quot;, \u0026quot;std.err\u0026quot;, \u0026quot;lower\u0026quot;, \u0026quot;upper\u0026quot;)]) #' Cut the value from the strata #' We split the strata to get variables rs.table_temp \u0026lt;- data.frame(do.call(rbind, strsplit(as.character(strsplit(as.character(rs.table$strata), \u0026quot;,\u0026quot;)),\u0026quot;=\u0026quot;, fixed=TRUE) )) rs.table_temp \u0026lt;- rs.table_temp %\u0026gt;% mutate(sex = substr(X2,1,1), subsite2 = substr(X3,1,1), subsite3 = substr(X4,1,1), subsite4 = substr(X5,1,1), Localised = substr(X6,1,1), Regional = substr(X7,1,1), Distant = substr(X8,1,1), )%\u0026gt;% select(sex, subsite2, subsite3, subsite4, Localised, Regional, Distant) rs.table \u0026lt;- cbind(rs.table[ ,-c(1, 9)], rs.table_temp) #' Take a look #' surv here is cumulative relative survival head(rs.table, 20) # Make plotdata for subsite=2 and Localised=1 rssurv.plotdata \u0026lt;- subset( rs.table, (subsite2 == 1 \u0026amp; Localised ==1) ) summary(rssurv.plotdata)  Plot net survival  ggplot(rssurv.plotdata, aes(x = time, y = surv, color = sex, fill = sex )) + geom_smooth(alpha = 0.25) + scale_x_continuous(breaks = seq(0, 10, by = 2), limits = c(0,10))+ scale_y_continuous(breaks = seq(0, 1.2, by = 0.2), limits = c(0,1.2))+ labs(title=\u0026quot;Cumulative relative survival for subsite=2 \u0026amp; stage =localised\u0026quot;, x=\u0026quot;Time since diagnosis (years)\u0026quot;, y=\u0026quot;Cumulative relative survival\u0026quot;) + theme(plot.title = element_text(size = 10))    Reference  Perme, MP, Pavlic, K. Nonparametric relative survival analysis with the R package relsurv. Journal of Statistical Software. 2018; 87(1), 1-27..  Appendix: codebook  sex: 1 (Male) 2 (Female) stage: 0 (Unknown); 1 (Localised); 2 (Regional); 3(Distant) subsite: 1(Coecum and ascending); 2 (Transverse); 3 (Descending and sigmoid); 4 (Other and NOS) staus: 0 (Alive); 1 (dead from colon cancer); 2 (dead from other causes)  ","date":1597795200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597795200,"objectID":"0eb759f1376993a8c5cf3d82daabdf0a","permalink":"https://enochytchen.com/tutorials/ratetable/content/","publishdate":"2020-08-19T00:00:00Z","relpermalink":"/tutorials/ratetable/content/","section":"tutorials","summary":"Enoch Chen, Paul Dickman\nThe syntax used in this tutorial can be found here.\nData preparation Install required packages and read the data. We are using a data set containing information on individuals diagnosed with colon cancer (because the data are publically available).","tags":null,"title":"Make ratetable using R","type":"docs"},{"authors":["Enoch Chen"],"categories":[],"content":"Table of Contents     Reference     I was pretty inspired by Anna Johansson\u0026rsquo;s workshop and course in good data management at Karolinska Institutet. Thus, I would like to share my experience along with my course notes in this blogpost. For the importance of good data management and my poor experience, please refer to my previous post: Data management? Does it matter?\nTip 1: Use a shared drive Spilling coffee on the laptop! The laptop was stolen! These kinds of poor stories we propbably have ever heard from someone else, or they even happened to us. Tragedies could happen, but using a shared drive could lower the impact it brings. Github, Tortoise, or other shared drives with a version control function can save you from the accident. It is recommended to commit your files to the shared drive once you finish the work everyday. Thus, you can always own a previous version of your files at the cloud drive or your colleagues\u0026rsquo;s drive (if they happen to use the same shared drive).\nYou commit to your job. Why don\u0026rsquo;t you commit your files as well? Establish a good habit of version control.\nTip 2: Give appropriate names to your files and variables Have you ever given your file an inappropriate name, and then you no longer found it?\n Don\u0026rsquo;t use stupid names, such as new1, new2, new3\u0026hellip;final1, final2, final3\u0026hellip;latest1\u0026hellip;\u0026hellip;A simple version mark plus date is quite enough (I usually use yyyymmdd)! E.g., manuscript_v1_20200501. No space in-between \u0026amp; No special character, such as Swedish vowels (ä, ö, å) or Mandarin words (請避免使用中文字). This is just to avoid that your file name is not readable to the software. For binomial variables, =1 implies yes, and =0 implies no. E.g., if a variable is entitled as female, then =1 indicates female, =0 indicates male. Convert a string variable to a numeric variable. Numeric varaibles are usually preferable for doing analysis. For example, in Stata, the conversion is done by destring (variable), replace.  Tip 3: Give the same names for linking files If you happen to organize the data, generate the log of the syntax, then output the result in word, it is recommended to use an identical name throughout these different files (.do .r .sas → .log → .doc). The priciple here is simply mark these files were basicaly processed in a series.\nTip 4: Don\u0026rsquo;t replace the original files/ vaiables if a new one is created It is quite common that new varivables are created during the data analysis. You should only create a new variable rather than overwrite the original variable. For example, age → nage . The same concept applies if you want to modify a file. However, if you accidentally overwrite the orginal file and you do have version control (Tip 1), there is still chance to trace it back.\nTip 5: Use file headers to document your work A file header plays a role as a reminder of what has been done in this single file. It allows us to understand that file by just reading the header. You yourself and your team members definitely say thank-you for keeping this record in the future.   Tip 6: Write notes while writing the codes My biostat teacher used to talk to himself while writing codes on his program file. He even wrote, What are we doing now? We are gonna...... I actually thought it was quite funny when he did that. But basically, telling yourself what you are doing is quite fundamental, since you will absolutely forget what you did. In addition, when I use new commands, I would also write down what it means, even though at that moment I caught the meaning of it. The same concept applies to keeping workbook or a working records. Believe me or not, your memories are not trustworthy! It is extra work definitely. Please do write something down and properly keep it.\nTip 7: Write a Readme.txt A Readme.txt functions as a table of contents. Basically, Readme should explain the contents in each folder, the author(s) of the work, and the modification that has been updated.\nImagine that you go to a restaurant, and you would like to order some dishes. Reasonably, you would ask for a menu. A Readme.txt is the menu of your folders or files. To be a good restaurant owner, would you run your business without a proper menu?\nReference  The Department of Medical Epidemiology and Biostatistics, Karolinska Institutet. MEB Guidelines for Documentation and Archiving Version 6. 2018. Johansson A. Data Management and research documentation for researchers. KIB Workshop 2018.  ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"b2b0516c7fb83b1f7e72c775e5c087c3","permalink":"https://enochytchen.com/post/gooddatamanagement/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/post/gooddatamanagement/","section":"post","summary":"Dos and don'ts of data management. Let's pick them up together.","tags":["data","SHLE series"],"title":"Seven tips of good data management","type":"post"},{"authors":["Enoch Chen"],"categories":[],"content":"Why should you do data management? We all have experienced unexpected shutdown of our laptops or even files we were still processing and haven\u0026rsquo;t saved yet. Maybe these nightmares are quite unavoidable sometimes; otherwise, I wouldn\u0026rsquo;t call it unexpected, and we would not have cursed as it happened.\nThe most attractive points of data management are\n To reproduce your work on analysis. Science is something that could be reproducible, so is DATA SCIENCE. Even though you might not work in a data-driven field, you definitely want to trace back your file records at a point. To work coherently and efficiently with yourself We all tend to forget where and how we put the files someday. Data management saves you a great amount of time of suffering from getting lost in the mess. To help your colleagues to understand the analysis It is quite common that if you work in a team, and someone joins after the project has already started, he or she needs to take over a part of the tasks. Then a good data managment gives that new colleague a big picture of what has been completed and what needs to be fixed. To enhance accuracy of work A good data management leads to a well-structured folders/file/codes/documents. Once any problem happens, it is easier to break down the error and find the bug.  My experience in poor data management Looking back my experience in managing data, I made a bunch of mistakes, which definitely annoyed my colleagues and stumbled myself. I don\u0026rsquo;t mind writing these bad examples down to highlight the importance of good data managment.\nProject died on the way I was once working in a project entitled \u0026ldquo;Enterovirus epidemic and class suspension in Taiwan\u0026rdquo;. That was the very first time that I had the access into governmental data. Without any analysis plan, I only used the cloud drive to keep the records of data analyses, which were also incomplete, with my supervisor. However, at the end, I was not able to continue the project and publish the results, so my supervisor took over the work. But when he asked me what we did together in the analyses and what extra work I managed to conduct on my own, I couldn\u0026rsquo;t properly answer the questions he proposed, because I did not have an analysis plan! All the data was carried out in an aggregation form, which indicated that we were unable to trace back to the working records in the data\u0026hellip;\u0026hellip;If I had learned data managment earlier, the project wouldn\u0026rsquo;t have to be halted.\nMy colleagues got mad The other experience also happened durign my undergrad. I was an NGO intern involved in dengue surveillance project in Northern Malawi. The project took much longer than we expected, so the next year\u0026rsquo;s interns also needed to participate in the unfinished project. However, we were the investigators at the first place. We understood all the project from the very beginning, but they did not. It took effort to explain what we had finished. At the end, even though we had quited the work, those next-years still did not catch the full results and working process of the project. And as I understood, they were not quite happy about the transition of the project. If I had learned data managment earlier, I would have had better transition of our work.\nSolution: good data management I picked up good data managment while working as a master thesis student at the Department of Medical Biostatistics and Epidemiology. I benefitted from the good data management workshop and courses offered by Anna Johansson and the data management group. I am quite sure given good data management, those two stories would not have occurred! Isn\u0026rsquo;t that a good news!?\nDo you have the same issure in your data management? Then you should learn Seven tips of good data management together.\nReference  Johansson A. Data Management and research documentation for researchers. KIB Workshop 2018.  ","date":1589587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589587200,"objectID":"8fc9eb5a1238f592f48b9a13bb49c5bf","permalink":"https://enochytchen.com/post/why_datamanagement/","publishdate":"2020-05-16T00:00:00Z","relpermalink":"/post/why_datamanagement/","section":"post","summary":"Data management is boring for sure. Take a look at my poor experience. Then you would agree it is essential. ","tags":["data"],"title":"Data management? Does it matter?","type":"post"},{"authors":["Enoch Chen"],"categories":[],"content":"Table of Contents    The UK\u0026rsquo;s life expectancy projection The Sweden\u0026rsquo;s life expectancy projection Reflection Reference     There are a bunch of quotes related to prediction which can be found at It’s Difficult to Make Predictions, Especially About the Future. Basically, these quotes do not differ from one another. They generally want to express that predection on future is risky and chanllenging. Moreover, it could lead to quantitative problems, such as underestimation or overestimation.\nRecently, while I was working on life expectancy estimation, a colleague proposed a quesiton, \u0026ldquo;If you are validating expected survival of patients in 1990s, shall you use the survival projection made in 1990s rather than the empircal survival?\u0026rdquo; As a person living in 2020, it was quite intuitive to me to use empirical data to validate the survival prediction. Nevertheless, practically, statisticians back to 1990s only had projection data. Based on this fact, I was quite convinced that I should have used the projection data made at that time.\nHowever, how precise was the data? Here come two small examples of validating life expectancy projection in the UK and Sweden.\nThe UK\u0026rsquo;s life expectancy projection The UK\u0026rsquo;s Office for National Statistics has ever published a report of validating the accurancy of life expectancy. The following figure was downloaded from National Population Projections Accuracy Report (1).\n  Actual and projected life expectancy at birth, UK, 1966-2030   It seems that the projection made before 2002 did not perform quite well. For example, projection made in 1975 saying males\u0026rsquo; life expectancy at birth was estimated to be 71 in 2001, whereas in 2001 it was actuall 79. A 8 years of difference! Then let\u0026rsquo;s take a look on the Swedish population.\nThe Sweden\u0026rsquo;s life expectancy projection Statistiska centralbyrån (Statistics Sweden) also published a similar report as the UK did to evaluate their previous projection on life expectancy. The following figure was downloaded from Sveriges framtida befolkning 2012–2060(2).\n  Actual and projected life expectancy at birth, Sweden, 1950-2012   The similar issue happened here as well. Earlier projection (before 2000) was not quite right in comparison with the empirical one.\nThis report was done in 2012 (the empirical trend until 2012). I then followed what Statistics Sweden conducted to generate a similar graph with the earliest open-access projection data till 2009 and empirical data till 2018 using data from Statistics Sweden (3) as well.\n  Actual and projected life expectancy at birth, Sweden, 2000-2030.   The difference was not as large as projection before 2000, but still, there could be 1-2 years of difference if projecting to 2030.\nReflection The validation between the empirical and the projection makes me aware that the longer we predict, the more potential bias may happen as well. This is just an example of why projection into the future could be adventurous. Imagine that if it is already uncertain in life expectancy prediction of the general population, would it be more difficult to predict a group of people\u0026rsquo;s (such as a patient cohort\u0026rsquo;s) survival? For sure, I would pretty much say yes because multiple factors can all contribute to the change of that specific group of people, like advancement of treatment, geographical difference, healthcare delievery, etc.\nThen shall we give up projection? I would however say no. I believe making prediction on something is better than knowing nothing and going ahead, even though people can always argue that we are forecasting the future and say, \u0026ldquo;You never know.\u0026rdquo; Well, at least having a reference point is better than none, isn\u0026rsquo;t it? Still, I agree that there is no perfect prediction. Who knows what is happening tomorrow? However, prediction made diligently is more believable than casually forecasting the future.\nDoesn\u0026rsquo;t my job sound like a fortune teller?\nReference  Office for National Statistics, UK. National Population Projections Accuracy Report, 2015. Available from: https://www.ons.gov.uk/ Statistiska centralbyrån (Statistics Sweden). Sveriges framtida befolkning 2012–2060 (Sweden\u0026rsquo;s future population), 2012. Available from http://share.scb.se/ Statistiska centralbyrån (Statistics Sweden). Statistical database: Earlier projections on life expectancy by sex and age. Available from: http://www.statistikdatabasen.scb.se/pxweb/en/ssd/  ","date":1589287936,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589287936,"objectID":"0b5cc28defe885942967500d3ad744cd","permalink":"https://enochytchen.com/post/prediction/","publishdate":"2020-05-12T14:52:16+02:00","relpermalink":"/post/prediction/","section":"post","summary":"Is it easy to make prediction about the future? Take a look at the UK's and Sweden's life expectancy projection.","tags":["expected survival","life expectancy","Sweden","UK"],"title":"Not easy to be a fortune teller: prediction is hard","type":"post"},{"authors":null,"categories":null,"content":"The codes used in this post are available here.\nThis blogpost introduces how to extrapolate patients\u0026rsquo; survival using flexible parametric model with age groups (categorical variable) as covariates. The tutorial is following the post Predicting in a new data set with stpm2 from Paul Dickman\u0026rsquo;s website, where he introduced how to extrapolate all-cause survival using flexible parametric model with age (continuous variable) and sex.\nThe following extrapolation results can then answer questions such as\n What is the estimated survival of patients aged 50-59 diagnosed with colon cancer during 1975-1985? What is the estimated survival of patients aged \u0026lt;50 diagnosed with breast cancer during 2000-2010?  Data preparation First you need to prepare the data before start modeling with stpm2. The exmaple colon cancer data used here can be found from my previous post: Data preparation.\nuse colon1975_1985,clear  stset stime, failure(dead==1,2) id(id) exit(time 10)  We stset the data for survival analysis. In this dataset, death from cancer was coded as dead=1 and from other causes as dead=2. We are interested in patients who died from any cause. We then assumed the maximum follow-up was 10 years, so exit(time 10) was set for making everyone censored after 10 years.\nModeling with stpm2 stpm2 agegroup2-agegroup5, /// agegroup0 is reference group scale(hazard) df(5) eform /// hazard: propotional hazard tvc(agegroup2-agegroup5) dftvc(2) // tvc allows non-proportional hazard  We then modeled the survival data in the flexible parametric model using stpm2. Basically, the setting showed that it is a proportional hazard model using scale(hazard), but we allowed time-dependent effects on the age group by using tvc(agegroup2-agegroup5).\nclear range agegroup 0 4 5 range _t 0 20 241 // (12*20)+1=241 fillin agegroup _t drop if missing(agegroup,_t) quietly tab agegroup, gen(agegroup) predict s, survival timevar(_t) sort agegroup _t save colon1975_1985_10yr_extrap, replace  As the model was stored in the background, we first cleared the data but created a new dataset (12 months*20years + the last point) for prediction by using fillin function. We then predict the survival by age group using predict to gain extrapolated values beyond 10 year of follow-up.\n  Extrapolation by restricting follow-up data to 10 years   The survival was by age groups, where the first 10 years were observed survival and later came the predicted survival.\n  Comparing extrapolation with empirical K-M curve   To evaluate the extrapolated survival, we then compared it with the empirical survival estimated by the Kaplan-Meier\u0026rsquo;s (K-M) method.\nIt seemed that for certain age groups in this dataset, the extrapolation would deviate from the empirical K-M curve. To obtain robust prediction, we prefer borrowing external information, such as vital statistics, to extrapolate the survival in relative survival framework rather than all-cause survival. Please refer to the next post: Extrapolating survival (relative survival framework) .\n","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587513600,"objectID":"8497f60ffda1ca737b0997d397190898","permalink":"https://enochytchen.com/tutorials/extrapolation/allcause/article/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/tutorials/extrapolation/allcause/article/","section":"tutorials","summary":"Extrapolating all-cause survival is straightforward, but how good is the preformance?","tags":null,"title":"Extrapolating survival (all-cause survival framework)","type":"docs"},{"authors":null,"categories":null,"content":"The codes used in this post are available here.\n// This data was downlaoded from Paul Dickman's website // Diagnosis 1975-1985 use http://enochytchen.com/directory/data/colon.dta if yydx\u0026gt;=1975 \u0026amp; yydx\u0026lt;=1985, clear  The data is originally from https://pauldickman.com/survival/colon.dta, permited use by Paul Dickman.\n// Rename variables rename yydx yeardiag rename age agediag gen stime=surv_mm/12 rename status dead  The varaibles are renamed for further use in the upcoming analysis.\n// Smart way to creating age groups egen agegroup=cut(agediag), at(0 50 60 70 80 200) icodes label variable agegroup \u0026quot;Age group\u0026quot; label define agegroup 0 \u0026quot;0-49\u0026quot; 1 \u0026quot;50-59\u0026quot; 2 \u0026quot;60-69\u0026quot; 3 \u0026quot;70-79\u0026quot; 4 \u0026quot;80+\u0026quot; label values agegroup agegroup // Create dummy varaiables for age categories quietly tab agegroup, gen(agegroup) save colon1975_1985,replace  Patients were categorized by age group \u0026lt;50, 50-59, 60-69, 70-79, \u0026gt;=80. Dummy variables on age group were created for modeling in stpm2. Then save the file for further modeling.\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"79308341b7b03e3ce28035fa0a31128a","permalink":"https://enochytchen.com/tutorials/extrapolation/data_prep/data_preparation/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/tutorials/extrapolation/data_prep/data_preparation/","section":"tutorials","summary":"Preparation for data used in upcoming analyses.","tags":null,"title":"Data clearance","type":"docs"},{"authors":null,"categories":null,"content":"The codes used in this post are available here\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"e62672a81a93a2cc1b7682d6c425eaa6","permalink":"https://enochytchen.com/tutorials/extrapolation/relative/make_expected_survival/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/tutorials/extrapolation/relative/make_expected_survival/","section":"tutorials","summary":"Prepare expected survival corresponding to the index patient cohort by age, sex, and calendar year using strs.","tags":null,"title":"Make expected survival","type":"docs"},{"authors":null,"categories":null,"content":"The codes used in this post are available here\nData preparation First you need to prepare the data before start modeling with stpm2. The exmaple data used here can be found from my previous post: Data preparation.\nExpected survival Other than modeling on patient survival, $S(t)$, it is more plausible to borrow external information, such as vital statistics, to obtain expected survival, $S^{*}(t)$, to extrapolate survival within relative survival framework.\n$R(t)=\\frac{S(t)}{S^{*}(t)}$\nThe analogue of relative survival, $R(t)$, on a hazard scale is\n$h(t)=h^{*}(t)+\\lambda(t)$\nBy integration, the result is\n$H(t)=H^{*}(t)+\\Lambda(t),$\nwhere the cumulative hazard, $H(t)$, consists of the cumulative expected hazard, $H^{*}(t)$, and the cumulative excess hazard, $\\Lambda(t)$.\nThe cumulative expected hazard can be obtained from population mortality. For how to prepare life table data for expected survival, the hand-in-hand teaching materials can be found at my previous posts: Population mortality rate projection and Prepare expected survival. colon_expected_survival.dta and popmort_projection.dta, both of which will be used in the following modeling process, will be generated after running the do files.\nAfter running the above programs, we then acquire the expected survival corresponding to the patient cohort\u0026rsquo;s survival by age, sex, and calendar year. Here we start model the relative survival.\nCreating relative survival use colon1975_1985,clear  stset stime, failure(dead==1,2) id(id) exit(time 10)  We stset the data for survival analysis. In this dataset, death from cancer was coded as dead=1 and from other causes as dead=2. We are interested in patients who died from any cause. We then assumed the maximum follow-up was 10 years, so exit(time 10) was set for making everyone censored after 10 years.\ngen _year= floor(min(yeardiag + _t, 1995)) gen _age=floor(min(agediag + _t, 105)) merge m:1 _year sex _age /// using popmort_projection.dta, /// nolabel keep(match master) keepusing(rate) drop _age _year _merge  _year and _age were created corresponding to the life table. The maximum of _year was set as 1995, and _age as 105. Then the patients\u0026rsquo; survival data was merged with life table to obtain the expected mortality rate.\nModeling relative survival with stpm2 stpm2 agegroup2-agegroup5, /// scale(hazard) df(5) eform /// tvc(agegroup2-agegroup5) dftvc(2) bhazard(rate)  Rather than modeling all-cause survival, we specified using bhazard(rate) for modeling relative survival on log cumulative excess hazard using stpm2.\nclear range agegroup 0 4 5 range _t 0 20 241 // (12*20)+1=241 fillin agegroup _t drop if missing(agegroup,_t) quietly tab agegroup, gen(agegroup) predict rs, survival timevar(_t) sort agegroup _t save colon_10yr_rs_extrap, replace  Still a empty dataset was created for extrapolation. We then predict the relative survival by age group using predict to gain extrapolated values beyond 10 year of follow-up and save the extrapolated relative survival.\nSo far we have already successfully predicted relative survival. Looking back to the equation of relative survival, R(t), it can be easily transformed back to the all-cause survival, $S(t)$, by multiplying with expected survival, $S^*(t)$.\n$S(t)=R(t) \\times S^{*}(t)$\nuse colon_expected_survival, clear sort agegroup _t merge m:1 agegroup _t /// using colon_10yr_rs_extrap, /// nolabel keep(match master) keepusing(rs) //S(t)=S^*(t) * R(t) //cp=cp_e2*cr_e2 drop _merge gen obs=cp_e2*rs replace obs=1 if _t==0 sort agegroup _t save colon_10yr_rs_to_allcause, replace     Extrapolation by restricting follow-up data to 10 years   The survival was by age groups, where the first 10 years were observed survival and later came the predicted survival. To compare with the empirical Kaplan-Meier curves, the figure below demonstrates the extrapolation.\n  Comparing extrapolation with empirical K-M curve   Extrapolation within relative survival framework is more robust than all-cause survival. In comparison with the figure in Extrapolating survival with stpm2 (Part 1), we could observe that the extrapolated survival curves deviate less from the empirical K-M curves.\nYou may wonder it does not seem to be much different from each other. However, due to the maximum 20 years of follow-up in this dataset, we were not able to generate the empirical curve beyond 20 years. If another dataset containing a longer follow-up period is used, we project that extrapolation in all-cause survival would deviate more, resulting uncertainty in prediction.\nSo far we have briefly compare the difference between extrapolating survival in all-cause survival and in relative survival. But what makes relative survival more plausible for extrapolation? Please check the next post: Why makes extrapolating relative survival more precise? for explanation!\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"e1a18ff4465cb713556b4b2683c97796","permalink":"https://enochytchen.com/tutorials/extrapolation/relative/article/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/tutorials/extrapolation/relative/article/","section":"tutorials","summary":"Extrapolating relative survival and transforming it back to all-cause survival by mutiplying the expected survival.","tags":null,"title":"Extrapolating survival (relative survival framework)","type":"docs"},{"authors":null,"categories":null,"content":"The codes used in this post are available here\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"f9f2a790b3c18d66e940517b9aa38619","permalink":"https://enochytchen.com/tutorials/extrapolation/data_prep/popmort_projection/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/tutorials/extrapolation/data_prep/popmort_projection/","section":"tutorials","summary":"In expected survival, population motality projection is often required for future survival.","tags":null,"title":"Population mortality projection","type":"docs"}]