[{"authors":["admin"],"categories":null,"content":"I am a Research Assistant of Biostatistics in the Biostatistics Group at the Department of Medical Epidemiology and Biostatistics, Karolinska Institutet. My main research interests are developing and applying statistical approaches for register-based epidemiological studies, particularly in survival analysis and cost-effectiveness studies in health technology assessment. I enjoy teaching statistics and telling stories as well. I truly hope you find my personal website useful to you.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://enochytchen.com/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a Research Assistant of Biostatistics in the Biostatistics Group at the Department of Medical Epidemiology and Biostatistics, Karolinska Institutet. My main research interests are developing and applying statistical approaches for register-based epidemiological studies, particularly in survival analysis and cost-effectiveness studies in health technology assessment.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"Stories behind this tutorial I wrote these tutorial while conducting my MSc thesis project in validating different survival extrapolation methods. During my work as a master student at Karolinska Institutet, I especially benefitted from Paul Dickman\u0026lsquo;s Stata teaching tutorials, Therese Andersson\u0026lsquo;s article on Statistics in Medicine (2013) and her PhD thesis, Paul Lambert\u0026lsquo;s Interactive graphs of explaining spline functions , Sandra Eloranta and Hannah Bower\u0026lsquo;s previous analysis files. It took me time and effort to find the right way to organize and generate the information needed for extrapolation. Thus, I would like to share with people who are eager to understand how to extrapolate survival and estimate life expectancy by using flexible parametric models (stpm2 package in Stata).\n","date":1588118400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1588118400,"objectID":"d73c108959a4b69e927b2353bbeddaab","permalink":"https://enochytchen.com/tutorials/extrapolation/","publishdate":"2020-04-29T00:00:00Z","relpermalink":"/tutorials/extrapolation/","section":"tutorials","summary":"Learn how to extrapolate survival data in Stata","tags":null,"title":"Survival extrapolation using Stata","type":"docs"},{"authors":null,"categories":null,"content":"Author: Enoch Chen, Paul Dickman\nIntroduction Population mortality rate tables (life tables) are typically stratified by age, sex, and year, and tabulated by government statistics offices (e.g., Human Mortality Database available at https://www.mortality.org/). Occasionally, we would also aim to stratify the rate table by other factors (e.g., race, social class, comorbidity, etc). This tutorial illustrates how to create a rate table from individual data for a cohort representative of the target population (i.e., a random sample of individuals without a diagnosis of cancer).\nData on a cohort of patients with colon cancer diagnosis is used as an example dataset in this tutorial. The original data is available (in Stata format) at https://pauldickman.com/data/colon.dta, whereas it is stored in the directory of this website as well.\nSteps in a nutshell  Model survival in multiple time scale flexible parametric model, where age is modeled continuously, and calendar year is a time-split variable. Save the predicted rates for each combination of variables in a data frame. In this example, besides sex, age, and calendar year, we use other two dimensions (subsite + stage) as well. Split the data frame of this mortality rate table into lists in order to apply transrate() and joinrate() to make a ratetable format in R. Use rs.surv()  along with the rate table to estimate net survival.  Acknowledgements We especially want to thank Alexander Ploner for providing statistical consulting and R code optimisation tips, Nurgul Batyrbekova for her insight in modeling survival data on multiple time scale, Joshua Entrop for his teaching in plotting regression model results, and Quang Thinh Trac for his assistance in optimising the syntax.\n","date":1597795200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1597795200,"objectID":"b5f9bd09aab26f10b26aafe7be7e9c66","permalink":"https://enochytchen.com/tutorials/ratetable/","publishdate":"2020-08-19T00:00:00Z","relpermalink":"/tutorials/ratetable/","section":"tutorials","summary":"Learn how to make ratetable based on multiple time-scale flexible parametric models in R","tags":null,"title":"Make ratetable using R","type":"docs"},{"authors":null,"categories":null,"content":"The syntax used in this tutorial can be found here.\nData preparation First, please install and use the packages needed in the following syntax, and read the colon data.\n#' 1 Preparation #' Clear all #' Use it if you need to clear all #' rm(list = ls()) #' Load required packages x\u0026lt;-c( \u0026quot;haven\u0026quot;, # read.dta() \u0026quot;tidyverse\u0026quot;, # dplyr::mutate \u0026quot;lubridate\u0026quot;, # decimal_date \u0026quot;survival\u0026quot;, # Surv(), survSplit() \u0026quot;rstpm2\u0026quot;, # stpm2() \u0026quot;splines\u0026quot;, # nsx() \u0026quot;relsurv\u0026quot;, # transrate(), joinrate(), rs.surv() \u0026quot;popEpi\u0026quot;, # as.data.frame(ratetable) \u0026quot;ggplot2\u0026quot;) # ggplot() lapply(x, require, character.only = TRUE) #' Read the data from web colon \u0026lt;- read_dta( \u0026quot;http://enochytchen.com/directory/data/colon.dta\u0026quot;) str(colon)  We then exacted the birth date of each observation by using dx (date of diagnosis) - age at diagnosis * 365.241. The unit here is days in order to be consistent with the unit in relsurv::rs.surv(). Variables sex, subsite, stage, and strata were converted into factor class, which will be automatically converted into dummy varaibles in a regression model in R. In this example, we used two dimensions: subsite + stage, and combined them into one stratum, a combination of all the dimensions except sex, age, and year. Splitting the data frame into lists will be done on this stratum afterwards. However, if there is only one extra dimension, it is not required to create a stratum. Instead, it is straightforward to split on that single variable.\n#' Extract the date of birth, #' Will take care of derived variables (entry/exit years) later colon2 \u0026lt;- colon %\u0026gt;% mutate(sex = as_factor(sex), ## as_factor preserves labels status = as.numeric(status), subsite = as_factor(subsite), ## as_factor preserves labels stage = as_factor(stage), ## as_factor preserves labels strata = as_factor(paste(subsite, stage, sep = \u0026quot;, \u0026quot;)), # strata consists all the dimensions dob = as.Date(dx) - age*365.241, ) %\u0026gt;% select(id, sex, status, subsite, stage, strata, dob, dx, exit, age) str(colon2) summary(colon2)  Splitting the time Since the secondary time scale in this example is calendar year, we splitted the survival data time at specified times (2 years). One-year interval would lead to convergence problem on stpm2 due to insufficient sample size within each time interval, so we splitted into 2-year intervals instead.\nIt is worth taking a look at the definition of episode in survival::survSplit, where it explains, \u0026ldquo;episode 1= less than the first cutpoint, 2= between the first and the second.\u0026rdquo; Based on this default setting, we then moved each episode (period) by 2.\n#' Split calendar time into 2-year intervals; #' splitting in 1-year intervals leads to convergence problems later if #' stpm2 does not have enough events within each time interval colon_split \u0026lt;- survSplit(Surv(decimal_date(dx), decimal_date(exit), status, type = \u0026quot;mstate\u0026quot;) ~ ., data = colon2, cut = seq(1975, 1995, by = 2), event = \u0026quot;status\u0026quot;, episode = \u0026quot;period\u0026quot; ) %\u0026gt;% # changed word \u0026quot;censor\u0026quot; to 0, so to keep it consistent with original definition mutate(status = as.numeric(ifelse(as.character(status) == \u0026quot;censor\u0026quot;, \u0026quot;0\u0026quot;, as.character(status))) ) #' Inspect: select the first 20 to take a look head(colon_split, 20) #' For downstream analysis, we want age as primary time scale; #' Calculate age at entry and age at exit #' Also, it would be nice to have the period expressed as actual starting year #' of the time interval; see ?survSplit for its definition #' (i.e. 1 = before first interval) colon_split = mutate(colon_split, age_start = tstart - decimal_date(dob), age_stop = tstop - decimal_date(dob), period = 1975 + (period - 2)*2) # *2 for 2-year intervals #' Save the data for running AIC/BIC test #' saveRDS(colon_split, \u0026quot;./Data/split_colon.rds\u0026quot;)  Flexible parametric model The colon cancer patients\u0026rsquo; data was modelled in multiple time scales, where the primary time scale is attained age calculated from two time points (age_start and age_stop), and the secondary is time-split calendar year fitted a natural spline using nsx() . Time-dependent effect of explanatory varibles was taken into consideration specifying tvc=list().\nfpm \u0026lt;- stpm2(Surv(time = age_start, time2 = age_stop, event = status==2) ~ sex + nsx(period, df=2) + subsite+ stage, data = colon_split, tvc = list(sex=3, period=2), df = 3) summary(fpm) #' A nice hazard plot #' Age-specific hazard by sex, subsite=2, stage =1, 1980 newdata = data.frame(sex = levels(colon2$sex), subsite = \u0026quot;Descending and sigmoid\u0026quot;, stage = \u0026quot;Localised\u0026quot;, period = 1980) newdata plot(fpm, newdata = newdata[2,,drop=FALSE], type = \u0026quot;hazard\u0026quot;, ci = FALSE, xlim = c(40,110), xlab = \u0026quot;Attained age (years)\u0026quot;, ylim = c(1E-6,100), ylab = \u0026quot;Hazard (log-scale)\u0026quot;, log = \u0026quot;y\u0026quot;, main = \u0026quot;Age-specific log-hazard for subsite=2, stage =1, period 1980, by sex\u0026quot;) lines(fpm, newdata = newdata[1,,drop=FALSE], type = \u0026quot;hazard\u0026quot;, lty = 2, col = \u0026quot;red\u0026quot;) legend(\u0026quot;topleft\u0026quot;, legend=c(\u0026quot;Female\u0026quot;, \u0026quot;Male\u0026quot;), col=c(\u0026quot;black\u0026quot;, \u0026quot;red\u0026quot;), lty=1:2)    Predicted rates in a data frame An empty data frame was generated using expand.grid() stratified by all the dimensions: sex, age_stop (age), period (year), subsite, and stage. The names of the variables must be consistent with which in the flexible parametric model. Then, predict() was applied to obtain predicted rates (i.e.,survival) in a data frame, similar to the concept in the extrapolation example using predict in Stata: Extrapolating survival (all-cause survival framework).\ncolon_new \u0026lt;- expand.grid(sex = levels(colon2$sex), subsite = levels(colon2$subsite), stage = levels(colon2$stage), age_stop = 1:110, period = 1975:1995) #'Populate the empty data frame with predicted hazards (based on the fitted model) colon_new$hazard \u0026lt;- predict(fpm, newdata = colon_new, type = \u0026quot;hazard\u0026quot;) colon_new \u0026lt;- colon_new %\u0026gt;% mutate(prob = exp(-hazard), age = age_stop, ) %\u0026gt;% select(sex, subsite, stage, age, period, prob, hazard) str(colon_new) #' Take look at the first 20 rows head(colon_new, 20)  Create ratetable Here comes the highlight of this tutorial!\nThe target focuses on how to transform the mortality rate dataframe into to a ratetable class data, which is required in rs.surv(). The subset of the mortality rate data frame, popmort_new, consists of age, sex, period, strata, and prob (survival probability).\nFirst, we splitted the data frame into lists by the varaible strata (subsite + stage in this example. i.e., all the combinations of the extra dimensions). Second, we splitted the lists by sex, so we are supposed to obtain the lists stratified by both strata and sex.\nspread() was then applied to transpose the matricies. Afterwards, transrate() combined the lists stratified by sex and joinrate() made the lists stratified by strata into a ratetable.\npopmort_new \u0026lt;- colon_new %\u0026gt;% mutate(strata = paste(subsite, stage, sep = \u0026quot;, \u0026quot;), )%\u0026gt;% select(age, sex, period, strata, prob) #' transrate() wants two matrices, both age x year, one for men, one for #' women; using split() repeatedly to make it work #' split() converted a data.fram into lists based on the specified variable #' First, split our popmort file by strata pm_split = split(popmort_new[, -4], popmort_new$strata) str(pm_split) #' Then we split the list again by sex pm_split = lapply(pm_split, function(x) split(x[, -2], x$sex)) str(pm_split) #' Using spread + as.matrix to generate the input matrices that transrate() #' needs spread_df = function(x) { ret = spread(x, period, prob) rownames(ret) = ret$age - 1 # Drop the age variable ret = ret[, -1] as.matrix( ret ) } pm_split = lapply(pm_split, function(x) lapply(x, spread_df )) str(pm_split) #' Now do the transrate() for each strar=ta; we get a list of #' ratetable-objects pm_split = lapply(pm_split, function(x) transrate(x$Male, x$Female, yearlim = c(1975, 1995))) str(pm_split) #' We can directly use the jointable-command on this list myratetable \u0026lt;- joinrate(pm_split, dim.name=\u0026quot;strata\u0026quot;) str(myratetable) #' Check whether is a readable ratetable for rs.surv() is.ratetable(myratetable)  Use rs.surv() to estimate net survival It is important to keep in mind that in relsuv::rs.surv() the unit of follow-up time is specified in days (1). Thus, given that the time in the original colon dataset is in years, we need to multiply the time by 365.241. The same rule applies to age, where in relsuv::rs.surv()::rmap age should be multiplied by 365.241 as well. The rate table was generated from this patient data. Therefore, if we estimate net survival using the same data, we should get relative survival close to 1, shown in the following syntax and graph.\nrssurv\u0026lt;-rs.surv(Surv(time = (decimal_date(exit)- decimal_date(dx)) * 365.241, event = status == 2) ~ sex + subsite+ stage, rmap = list(age = age*365.241, year = dx, strata = strata), ratetable = myratetable, data = colon2, method = \u0026quot;ederer2\u0026quot;) rssurv.sum \u0026lt;- summary(rssurv, time = c(0:10) * 365.241, scale = 365.241) rs.table \u0026lt;- as.data.frame(rssurv.sum[c(\u0026quot;strata\u0026quot;, \u0026quot;time\u0026quot;, \u0026quot;n.risk\u0026quot;, \u0026quot;n.event\u0026quot;, \u0026quot;surv\u0026quot;, \u0026quot;std.err\u0026quot;, \u0026quot;lower\u0026quot;, \u0026quot;upper\u0026quot;)]) #' Cut the value from the strata #' We split the strata to get variables rs.table_temp \u0026lt;- data.frame(do.call(rbind, strsplit(as.character(strsplit(as.character(rs.table$strata), \u0026quot;,\u0026quot;)),\u0026quot;=\u0026quot;, fixed=TRUE) )) rs.table_temp \u0026lt;- rs.table_temp %\u0026gt;% mutate(sex = substr(X2,1,1), subsite2 = substr(X3,1,1), subsite3 = substr(X4,1,1), subsite4 = substr(X5,1,1), Localised = substr(X6,1,1), Regional = substr(X7,1,1), Distant = substr(X8,1,1), )%\u0026gt;% select(sex, subsite2, subsite3, subsite4, Localised, Regional, Distant) rs.table \u0026lt;- cbind(rs.table[ ,-c(1, 9)], rs.table_temp) #' Take a look #' surv here is cumulative relative survival head(rs.table, 20) # Make plotdata for subsite=2 and Localised=1 rssurv.plotdata \u0026lt;- subset( rs.table, (subsite2 == 1 \u0026amp; Localised ==1) ) summary(rssurv.plotdata)  Plot net survival  ggplot(rssurv.plotdata, aes(x = time, y = surv, color = sex, fill = sex )) + geom_smooth(alpha = 0.25) + scale_x_continuous(breaks = seq(0, 10, by = 2), limits = c(0,10))+ scale_y_continuous(breaks = seq(0, 1.2, by = 0.2), limits = c(0,1.2))+ labs(title=\u0026quot;Cumulative relative survival for subsite=2 \u0026amp; stage =localised\u0026quot;, x=\u0026quot;Time since diagnosis (years)\u0026quot;, y=\u0026quot;Cumulative relative survival\u0026quot;) + theme(plot.title = element_text(size = 10))    Reference  Perme, MP, Pavlic, K. Nonparametric relative survival analysis with the R package relsurv. Journal of Statistical Software. 2018; 87(1), 1-27..  Appendix: codebook  sex: 1 (Male) 2 (Female) stage: 0 (Unknown); 1 (Localised); 2 (Regional); 3(Distant) subsite: 1(Coecum and ascending); 2 (Transverse); 3 (Descending and sigmoid); 4 (Other and NOS) staus: 0 (Alive); 1 (dead from colon cancer); 2 (dead from other causes)  ","date":1597795200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597795200,"objectID":"0eb759f1376993a8c5cf3d82daabdf0a","permalink":"https://enochytchen.com/tutorials/ratetable/content/","publishdate":"2020-08-19T00:00:00Z","relpermalink":"/tutorials/ratetable/content/","section":"tutorials","summary":"The syntax used in this tutorial can be found here.\nData preparation First, please install and use the packages needed in the following syntax, and read the colon data.\n#' 1 Preparation #' Clear all #' Use it if you need to clear all #' rm(list = ls()) #' Load required packages x\u0026lt;-c( \u0026quot;haven\u0026quot;, # read.","tags":null,"title":"Make ratetable using R","type":"docs"},{"authors":["Enoch Chen"],"categories":[],"content":"I was pretty inspired by Anna Johansson\u0026lsquo;s workshop and course in good data management at Karolinska Institutet. Thus, I would like to share my experience along with my course notes in this blogpost. For the importance of good data management and my poor experience, please refer to my previous post: Data management? Does it matter?\nTip 1: Use a shared drive Spilling coffee on the laptop! The laptop was stolen! These kinds of poor stories we propbably have ever heard from someone else, or they even happened to us. Tragedies could happen, but using a shared drive could lower the impact it brings. Github, Tortoise, or other shared drives with a version control function can save you from the accident. It is recommended to commit your files to the shared drive once you finish the work everyday. Thus, you can always own a previous version of your files at the cloud drive or your colleagues\u0026rsquo;s drive (if they happen to use the same shared drive).\nYou commit to your job. Why don\u0026rsquo;t you commit your files as well? Establish a good habit of version control.\nTip 2: Give appropriate names to your files and variables Have you ever given your file an inappropriate name, and then you no longer found it?\n Don\u0026rsquo;t use stupid names, such as new1, new2, new3\u0026hellip;final1, final2, final3\u0026hellip;latest1\u0026hellip;\u0026hellip;A simple version mark plus date is quite enough (I usually use yyyymmdd)! E.g., manuscript_v1_20200501. No space in-between \u0026amp; No special character, such as Swedish vowels (ä, ö, å) or Mandarin words (請避免使用中文字). This is just to avoid that your file name is not readable to the software. For binomial variables, =1 implies yes, and =0 implies no. E.g., if a variable is entitled as female, then =1 indicates female, =0 indicates male. Convert a string variable to a numeric variable. Numeric varaibles are usually preferable for doing analysis. For example, in Stata, the conversion is done by destring (variable), replace.  Tip 3: Give the same names for linking files If you happen to organize the data, generate the log of the syntax, then output the result in word, it is recommended to use an identical name throughout these different files (.do .r .sas → .log → .doc). The priciple here is simply mark these files were basicaly processed in a series.\nTip 4: Don\u0026rsquo;t replace the original files/ vaiables if a new one is created It is quite common that new varivables are created during the data analysis. You should only create a new variable rather than overwrite the original variable. For example, age → nage . The same concept applies if you want to modify a file. However, if you accidentally overwrite the orginal file and you do have version control (Tip 1), there is still chance to trace it back.\nTip 5: Use file headers to document your work A file header plays a role as a reminder of what has been done in this single file. It allows us to understand that file by just reading the header. You yourself and your team members definitely say thank-you for keeping this record in the future.   Tip 6: Write notes while writing the codes My biostat teacher used to talk to himself while writing codes on his program file. He even wrote, What are we doing now? We are gonna...... I actually thought it was quite funny when he did that. But basically, telling yourself what you are doing is quite fundamental, since you will absolutely forget what you did. In addition, when I use new commands, I would also write down what it means, even though at that moment I caught the meaning of it. The same concept applies to keeping workbook or a working records. Believe me or not, your memories are not trustworthy! It is extra work definitely. Please do write something down and properly keep it.\nTip 7: Write a Readme.txt A Readme.txt functions as a table of contents. Basically, Readme should explain the contents in each folder, the author(s) of the work, and the modification that has been updated.\nImagine that you go to a restaurant, and you would like to order some dishes. Reasonably, you would ask for a menu. A Readme.txt is the menu of your folders or files. To be a good restaurant owner, would you run your business without a proper menu?\nReference  The Department of Medical Epidemiology and Biostatistics, Karolinska Institutet. MEB Guidelines for Documentation and Archiving Version 6. 2018. Johansson A. Data Management and research documentation for researchers. KIB Workshop 2018.  ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"b2b0516c7fb83b1f7e72c775e5c087c3","permalink":"https://enochytchen.com/post/gooddatamanagement/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/post/gooddatamanagement/","section":"post","summary":"Dos and don'ts of data management. Let's pick them up together.","tags":["data","SHLE series"],"title":"Seven tips of good data management","type":"post"},{"authors":[],"categories":[],"content":"Why should you do data management? We all have experienced unexpected shutdown of our laptops or even files we were still processing and haven\u0026rsquo;t saved yet. Maybe these nightmares are quite unavoidable sometimes; otherwise, I wouldn\u0026rsquo;t call it unexpected, and we would not have cursed as it happened.\nThe most attractive points of data management are\n To reproduce your work on analysis. Science is something that could be reproducible, so is DATA SCIENCE. Even though you might not work in a data-driven field, you definitely want to trace back your file records at a point. To work coherently and efficiently with yourself We all tend to forget where and how we put the files someday. Data management saves you a great amount of time of suffering from getting lost in the mess. To help your colleagues to understand the analysis It is quite common that if you work in a team, and someone joins after the project has already started, he or she needs to take over a part of the tasks. Then a good data managment gives that new colleague a big picture of what has been completed and what needs to be fixed. To enhance accuracy of work A good data management leads to a well-structured folders/file/codes/documents. Once any problem happens, it is easier to break down the error and find the bug.  My experience in poor data management Looking back my experience in managing data, I made a bunch of mistakes, which definitely annoyed my colleagues and stumbled myself. I don\u0026rsquo;t mind writing these bad examples down to highlight the importance of good data managment.\nProject died on the way I was once working in a project entitled \u0026ldquo;Enterovirus epidemic and class suspension in Taiwan\u0026rdquo;. That was the very first time that I had the access into governmental data. Without any analysis plan, I only used the cloud drive to keep the records of data analyses, which were also incomplete, with my supervisor. However, at the end, I was not able to continue the project and publish the results, so my supervisor took over the work. But when he asked me what we did together in the analyses and what extra work I managed to conduct on my own, I couldn\u0026rsquo;t properly answer the questions he proposed, because I did not have an analysis plan! All the data was carried out in an aggregation form, which indicated that we were unable to trace back to the working records in the data\u0026hellip;\u0026hellip;If I had learned data managment earlier, the project wouldn\u0026rsquo;t have to be halted.\nMy colleagues got mad The other experience also happened durign my undergrad. I was an NGO intern involved in dengue surveillance project in Northern Malawi. The project took much longer than we expected, so the next year\u0026rsquo;s interns also needed to participate in the unfinished project. However, we were the investigators at the first place. We understood all the project from the very beginning, but they did not. It took effort to explain what we had finished. At the end, even though we had quited the work, those next-years still did not catch the full results and working process of the project. And as I understood, they were not quite happy about the transition of the project. If I had learned data managment earlier, I would have had better transition of our work.\nSolution: good data management I picked up good data managment while working as a master thesis student at the Department of Medical Biostatistics and Epidemiology. I benefitted from the good data management workshop and courses offered by Anna Johansson and the data management group. I am quite sure given good data management, those two stories would not have occurred! Isn\u0026rsquo;t that a good news!?\nDo you have the same issure in your data management? Then you should learn Seven tips of good data management together.\nReference  Johansson A. Data Management and research documentation for researchers. KIB Workshop 2018.  ","date":1589587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589587200,"objectID":"8fc9eb5a1238f592f48b9a13bb49c5bf","permalink":"https://enochytchen.com/post/why_datamanagement/","publishdate":"2020-05-16T00:00:00Z","relpermalink":"/post/why_datamanagement/","section":"post","summary":"Data management is boring for sure. Take a look at my poor experience. Then you would agree it is essential. ","tags":["data"],"title":"Data management? Does it matter?","type":"post"},{"authors":[],"categories":[],"content":"There are a bunch of quotes related to prediction which can be found at It’s Difficult to Make Predictions, Especially About the Future. Basically, these quotes do not differ from one another. They generally want to express that predection on future is risky and chanllenging. Moreover, it could lead to quantitative problems, such as underestimation or overestimation.\nRecently, while I was working on life expectancy estimation, a colleague proposed a quesiton, \u0026ldquo;If you are validating expected survival of patients in 1990s, shall you use the survival projection made in 1990s rather than the empircal survival?\u0026rdquo; As a person living in 2020, it was quite intuitive to me to use empirical data to validate the survival prediction. Nevertheless, practically, statisticians back to 1990s only had projection data. Based on this fact, I was quite convinced that I should have used the projection data made at that time.\nHowever, how precise was the data? Here come two small examples of validating life expectancy projection in the UK and Sweden.\nThe UK\u0026rsquo;s life expectancy projection The UK\u0026rsquo;s Office for National Statistics has ever published a report of validating the accurancy of life expectancy. The following figure was downloaded from National Population Projections Accuracy Report (1).\n  Actual and projected life expectancy at birth, UK, 1966-2030   It seems that the projection made before 2002 did not perform quite well. For example, projection made in 1975 saying males\u0026rsquo; life expectancy at birth was estimated to be 71 in 2001, whereas in 2001 it was actuall 79. A 8 years of difference! Then let\u0026rsquo;s take a look on the Swedish population.\nThe Sweden\u0026rsquo;s life expectancy projection Statistiska centralbyrån (Statistics Sweden) also published a similar report as the UK did to evaluate their previous projection on life expectancy. The following figure was downloaded from Sveriges framtida befolkning 2012–2060(2).\n  Actual and projected life expectancy at birth, Sweden, 1950-2012   The similar issue happened here as well. Earlier projection (before 2000) was not quite right in comparison with the empirical one.\nThis report was done in 2012 (the empirical trend until 2012). I then followed what Statistics Sweden conducted to generate a similar graph with the earliest open-access projection data till 2009 and empirical data till 2018 using data from Statistics Sweden (3) as well.\n  Actual and projected life expectancy at birth, Sweden, 2000-2030.   The difference was not as large as projection before 2000, but still, there could be 1-2 years of difference if projecting to 2030.\nReflection The validation between the empirical and the projection makes me aware that the longer we predict, the more potential bias may happen as well. This is just an example of why projection into the future could be adventurous. Imagine that if it is already uncertain in life expectancy prediction of the general population, would it be more difficult to predict a group of people\u0026rsquo;s (such as a patient cohort\u0026rsquo;s) survival? For sure, I would pretty much say yes because multiple factors can all contribute to the change of that specific group of people, like advancement of treatment, geographical difference, healthcare delievery, etc.\nThen shall we give up projection? I would however say no. I believe making prediction on something is better than knowing nothing and going ahead, even though people can always argue that we are forecasting the future and say, \u0026ldquo;You never know.\u0026rdquo; Well, at least having a reference point is better than none, isn\u0026rsquo;t it? Still, I agree that there is no perfect prediction. Who knows what is happening tomorrow? However, prediction made diligently is more believable than casually forecasting the future.\nDoesn\u0026rsquo;t my job sound like a fortune teller?\nReference  Office for National Statistics, UK. National Population Projections Accuracy Report, 2015. Available from: https://www.ons.gov.uk/ Statistiska centralbyrån (Statistics Sweden). Sveriges framtida befolkning 2012–2060 (Sweden\u0026rsquo;s future population), 2012. Available from http://share.scb.se/ Statistiska centralbyrån (Statistics Sweden). Statistical database: Earlier projections on life expectancy by sex and age. Available from: http://www.statistikdatabasen.scb.se/pxweb/en/ssd/  ","date":1589287936,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589287936,"objectID":"0b5cc28defe885942967500d3ad744cd","permalink":"https://enochytchen.com/post/prediction/","publishdate":"2020-05-12T14:52:16+02:00","relpermalink":"/post/prediction/","section":"post","summary":"Is it easy to make prediction about the future? Take a look at the UK's and Sweden's life expectancy projection.","tags":["expected survival","life expectancy","Sweden","UK"],"title":"Not easy to be a fortune teller: prediction is hard","type":"post"},{"authors":null,"categories":null,"content":"The codes used in this post are available here.\nThis blogpost introduces how to extrapolate patients\u0026rsquo; survival using flexible parametric model with age groups (categorical variable) as covariates. The tutorial is following the post Predicting in a new data set with stpm2 from Paul Dickman\u0026lsquo;s website, where he introduced how to extrapolate all-cause survival using flexible parametric model with age (continuous variable) and sex.\nThe following extrapolation results can then answer questions such as\n What is the estimated survival of patients aged 50-59 diagnosed with colon cancer during 1975-1985? What is the estimated survival of patients aged \u0026lt;50 diagnosed with breast cancer during 2000-2010?  Data preparation First you need to prepare the data before start modeling with stpm2. The exmaple colon cancer data used here can be found from my previous post: Data preparation.\nuse colon1975_1985,clear  stset stime, failure(dead==1,2) id(id) exit(time 10)  We stset the data for survival analysis. In this dataset, death from cancer was coded as dead=1 and from other causes as dead=2. We are interested in patients who died from any cause. We then assumed the maximum follow-up was 10 years, so exit(time 10) was set for making everyone censored after 10 years.\nModeling with stpm2 stpm2 agegroup2-agegroup5, /// agegroup0 is reference group scale(hazard) df(5) eform /// hazard: propotional hazard tvc(agegroup2-agegroup5) dftvc(2) // tvc allows non-proportional hazard  We then modeled the survival data in the flexible parametric model using stpm2. Basically, the setting showed that it is a proportional hazard model using scale(hazard), but we allowed time-dependent effects on the age group by using tvc(agegroup2-agegroup5).\nclear range agegroup 0 4 5 range _t 0 20 241 // (12*20)+1=241 fillin agegroup _t drop if missing(agegroup,_t) quietly tab agegroup, gen(agegroup) predict s, survival timevar(_t) sort agegroup _t save colon1975_1985_10yr_extrap, replace  As the model was stored in the background, we first cleared the data but created a new dataset (12 months*20years + the last point) for prediction by using fillin function. We then predict the survival by age group using predict to gain extrapolated values beyond 10 year of follow-up.\n  Extrapolation by restricting follow-up data to 10 years   The survival was by age groups, where the first 10 years were observed survival and later came the predicted survival.\n  Comparing extrapolation with empirical K-M curve   To evaluate the extrapolated survival, we then compared it with the empirical survival estimated by the Kaplan-Meier\u0026rsquo;s (K-M) method.\nIt seemed that for certain age groups in this dataset, the extrapolation would deviate from the empirical K-M curve. To obtain robust prediction, we prefer borrowing external information, such as vital statistics, to extrapolate the survival in relative survival framework rather than all-cause survival. Please refer to the next post: Extrapolating survival (relative survival framework) .\n","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587513600,"objectID":"8497f60ffda1ca737b0997d397190898","permalink":"https://enochytchen.com/tutorials/extrapolation/allcause/article/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/tutorials/extrapolation/allcause/article/","section":"tutorials","summary":"Extrapolating all-cause survival is straightforward, but how good is the preformance?","tags":null,"title":"Extrapolating survival (all-cause survival framework)","type":"docs"},{"authors":null,"categories":null,"content":"The codes used in this post are available here.\n// This data was downlaoded from Paul Dickman's website // Diagnosis 1975-1985 use http://enochytchen.com/directory/data/colon.dta if yydx\u0026gt;=1975 \u0026amp; yydx\u0026lt;=1985, clear  The data is originally from https://pauldickman.com/survival/colon.dta, permited use by Paul Dickman.\n// Rename variables rename yydx yeardiag rename age agediag gen stime=surv_mm/12 rename status dead  The varaibles are renamed for further use in the upcoming analysis.\n// Smart way to creating age groups egen agegroup=cut(agediag), at(0 50 60 70 80 200) icodes label variable agegroup \u0026quot;Age group\u0026quot; label define agegroup 0 \u0026quot;0-49\u0026quot; 1 \u0026quot;50-59\u0026quot; 2 \u0026quot;60-69\u0026quot; 3 \u0026quot;70-79\u0026quot; 4 \u0026quot;80+\u0026quot; label values agegroup agegroup // Create dummy varaiables for age categories quietly tab agegroup, gen(agegroup) save colon1975_1985,replace  Patients were categorized by age group \u0026lt;50, 50-59, 60-69, 70-79, \u0026gt;=80. Dummy variables on age group were created for modeling in stpm2. Then save the file for further modeling.\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"79308341b7b03e3ce28035fa0a31128a","permalink":"https://enochytchen.com/tutorials/extrapolation/data_prep/data_preparation/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/tutorials/extrapolation/data_prep/data_preparation/","section":"tutorials","summary":"Preparation for data used in upcoming analyses.","tags":null,"title":"Data clearance","type":"docs"},{"authors":null,"categories":null,"content":"The codes used in this post are available here\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"e62672a81a93a2cc1b7682d6c425eaa6","permalink":"https://enochytchen.com/tutorials/extrapolation/relative/make_expected_survival/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/tutorials/extrapolation/relative/make_expected_survival/","section":"tutorials","summary":"Prepare expected survival corresponding to the index patient cohort by age, sex, and calendar year using strs.","tags":null,"title":"Make expected survival","type":"docs"},{"authors":null,"categories":null,"content":"The codes used in this post are available here\nData preparation First you need to prepare the data before start modeling with stpm2. The exmaple data used here can be found from my previous post: Data preparation.\nExpected survival Other than modeling on patient survival, $S(t)$, it is more plausible to borrow external information, such as vital statistics, to obtain expected survival, $S^{*}(t)$, to extrapolate survival within relative survival framework.\n$R(t)=\\frac{S(t)}{S^{*}(t)}$\nThe analogue of relative survival, $R(t)$, on a hazard scale is\n$h(t)=h^{*}(t)+\\lambda(t)$\nBy integration, the result is\n$H(t)=H^{*}(t)+\\Lambda(t),$\nwhere the cumulative hazard, $H(t)$, consists of the cumulative expected hazard, $H^{*}(t)$, and the cumulative excess hazard, $\\Lambda(t)$.\nThe cumulative expected hazard can be obtained from population mortality. For how to prepare life table data for expected survival, the hand-in-hand teaching materials can be found at my previous posts: Population mortality rate projection and Prepare expected survival. colon_expected_survival.dta and popmort_projection.dta, both of which will be used in the following modeling process, will be generated after running the do files.\nAfter running the above programs, we then acquire the expected survival corresponding to the patient cohort\u0026rsquo;s survival by age, sex, and calendar year. Here we start model the relative survival.\nCreating relative survival use colon1975_1985,clear  stset stime, failure(dead==1,2) id(id) exit(time 10)  We stset the data for survival analysis. In this dataset, death from cancer was coded as dead=1 and from other causes as dead=2. We are interested in patients who died from any cause. We then assumed the maximum follow-up was 10 years, so exit(time 10) was set for making everyone censored after 10 years.\ngen _year= floor(min(yeardiag + _t, 1995)) gen _age=floor(min(agediag + _t, 105)) merge m:1 _year sex _age /// using popmort_projection.dta, /// nolabel keep(match master) keepusing(rate) drop _age _year _merge  _year and _age were created corresponding to the life table. The maximum of _year was set as 1995, and _age as 105. Then the patients\u0026rsquo; survival data was merged with life table to obtain the expected mortality rate.\nModeling relative survival with stpm2 stpm2 agegroup2-agegroup5, /// scale(hazard) df(5) eform /// tvc(agegroup2-agegroup5) dftvc(2) bhazard(rate)  Rather than modeling all-cause survival, we specified using bhazard(rate) for modeling relative survival on log cumulative excess hazard using stpm2.\nclear range agegroup 0 4 5 range _t 0 20 241 // (12*20)+1=241 fillin agegroup _t drop if missing(agegroup,_t) quietly tab agegroup, gen(agegroup) predict rs, survival timevar(_t) sort agegroup _t save colon_10yr_rs_extrap, replace  Still a empty dataset was created for extrapolation. We then predict the relative survival by age group using predict to gain extrapolated values beyond 10 year of follow-up and save the extrapolated relative survival.\nSo far we have already successfully predicted relative survival. Looking back to the equation of relative survival, R(t), it can be easily transformed back to the all-cause survival, $S(t)$, by multiplying with expected survival, $S^*(t)$.\n$S(t)=R(t) \\times S^{*}(t)$\nuse colon_expected_survival, clear sort agegroup _t merge m:1 agegroup _t /// using colon_10yr_rs_extrap, /// nolabel keep(match master) keepusing(rs) //S(t)=S^*(t) * R(t) //cp=cp_e2*cr_e2 drop _merge gen obs=cp_e2*rs replace obs=1 if _t==0 sort agegroup _t save colon_10yr_rs_to_allcause, replace     Extrapolation by restricting follow-up data to 10 years   The survival was by age groups, where the first 10 years were observed survival and later came the predicted survival. To compare with the empirical Kaplan-Meier curves, the figure below demonstrates the extrapolation.\n  Comparing extrapolation with empirical K-M curve   Extrapolation within relative survival framework is more robust than all-cause survival. In comparison with the figure in Extrapolating survival with stpm2 (Part 1), we could observe that the extrapolated survival curves deviate less from the empirical K-M curves.\nYou may wonder it does not seem to be much different from each other. However, due to the maximum 20 years of follow-up in this dataset, we were not able to generate the empirical curve beyond 20 years. If another dataset containing a longer follow-up period is used, we project that extrapolation in all-cause survival would deviate more, resulting uncertainty in prediction.\nSo far we have briefly compare the difference between extrapolating survival in all-cause survival and in relative survival. But what makes relative survival more plausible for extrapolation? Please check the next post: Why makes extrapolating relative survival more precise? for explanation!\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"e1a18ff4465cb713556b4b2683c97796","permalink":"https://enochytchen.com/tutorials/extrapolation/relative/article/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/tutorials/extrapolation/relative/article/","section":"tutorials","summary":"Extrapolating relative survival and transforming it back to all-cause survival by mutiplying the expected survival.","tags":null,"title":"Extrapolating survival (relative survival framework)","type":"docs"},{"authors":null,"categories":null,"content":"The codes used in this post are available here\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"f9f2a790b3c18d66e940517b9aa38619","permalink":"https://enochytchen.com/tutorials/extrapolation/data_prep/popmort_projection/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/tutorials/extrapolation/data_prep/popmort_projection/","section":"tutorials","summary":"In expected survival, population motality projection is often required for future survival.","tags":null,"title":"Population mortality projection","type":"docs"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"be30009936fbce4f21910ac275796bc6","permalink":"https://enochytchen.com/tutorials/_index-/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tutorials/_index-/","section":"tutorials","summary":"","tags":null,"title":"Recent posts","type":"tutorials"}]